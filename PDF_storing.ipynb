{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "from pathlib import Path\n",
    "\n",
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we initialize Weaviate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ceciliaacosta/miniconda3/envs/ift-6758/lib/python3.11/site-packages/weaviate/warnings.py:158: DeprecationWarning: Dep016: You are using the Weaviate v3 client, which is deprecated.\n",
      "            Consider upgrading to the new and improved v4 client instead!\n",
      "            See here for usage: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "            \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded weaviate is already listening on port 8079\n"
     ]
    }
   ],
   "source": [
    "client = weaviate.Client(\n",
    "    embedded_options=EmbeddedOptions(\n",
    "        additional_env_vars={\"X-HuggingFace-Api-Key\": \"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\"}\n",
    "    )\n",
    ")\n",
    "assert client.is_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.schema.delete_all()\n",
    "# Create a new class with a vectorizer\n",
    "schema = {\n",
    "    \"class\": \"Test\",    \n",
    "    \"vectorizer\": \"text2vec-huggingface\",\n",
    "    \"properties\": [\n",
    "        {\n",
    "            \"name\": \"source\", \n",
    "            \"dataType\": [\"text\"],\n",
    "            \"description\": \"The source of the PDF\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"title\",\n",
    "            \"dataType\": [\"text\"],\n",
    "            \"description\": \"Titles in the PDF\",\n",
    "            \"moduleConfig\": {\n",
    "                \"text2vec-huggingface\": {\"skip\": False, \"vectorizePropertyName\": False}\n",
    "            },\n",
    "\n",
    "        },\n",
    "\n",
    "        {\n",
    "            \"name\": \"content\",  #What we want to vectorize\n",
    "            \"dataType\": [\"text\"],\n",
    "            \"description\": \"Content of PDF\",\n",
    "            \"moduleConfig\": {\n",
    "                \"text2vec-huggingface\": {\"skip\": False, \"vectorizePropertyName\": False}\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"path\",\n",
    "            \"dataType\": [\"text\"],\n",
    "            \"description\": \"The path to the PDF\"\n",
    "        },\n",
    "    ],\n",
    "    \"moduleConfig\": {\n",
    "    \"text2vec-huggingface\": {\n",
    "      \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",  # Can be any public or private Hugging Face model.\n",
    "      \"options\": {\n",
    "        \"waitForModel\": True,  # Try this if you get a \"model not ready\" error\n",
    "      }\n",
    "}\n",
    "}\n",
    "}\n",
    "\n",
    "client.schema.create_class(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "class TitleNarrativeExtractor:\n",
    "    def __init__(self):\n",
    "        self.titles = []  # Keep track of the extracted titles\n",
    "        self.narrative_texts = []  # Keep track of the extracted narrative texts\n",
    "\n",
    "    def process(self, element):\n",
    "        if element.category == \"Title\":\n",
    "            self.extract_title(element.text)\n",
    "\n",
    "        if element.category == \"NarrativeText\":\n",
    "            self.extract_narrative_text(element.text)\n",
    "\n",
    "    def extract_title(self, text):\n",
    "        logging.info(f\"Title extracted: {text}\")\n",
    "        self.titles.append(text)\n",
    "\n",
    "    def extract_narrative_text(self, text):\n",
    "        logging.info(f\"Narrative text extracted: {text}\")\n",
    "        self.narrative_texts.append(text)\n",
    "\n",
    "    def extract_elements(self, elements):\n",
    "        for element in elements:\n",
    "            self.process(element)\n",
    "\n",
    "    def get_titles(self):\n",
    "        return \"\\n\".join(self.titles)\n",
    "\n",
    "    def get_narrative_texts(self):\n",
    "        return \"\\n\".join(self.narrative_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pikepdf._core:pikepdf C++ to Python logger bridge initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing paper02.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "INFO:root:Title extracted: A Comparison of House Price Classification with Structured and Unstructured Text Data\n",
      "INFO:root:Title extracted: Erika Cardenas Florida Atlantic University ecardenas2015@fau.edu\n",
      "INFO:root:Title extracted: Connor Shorten Florida Atlantic University cshorten2015@fau.edu\n",
      "INFO:root:Title extracted: Taghi M. Khoshgoftaar Florida Atlantic University khoshgof@fau.edu\n",
      "INFO:root:Title extracted: Borivoje Furht Florida Atlantic University bfurht@fau.edu\n",
      "INFO:root:Title extracted: Abstract\n",
      "INFO:root:Narrative text extracted: Purchasing a home is one of the largest investments most people make. House price prediction allows indi- viduals to be informed about their asset wealth. Trans- parent pricing on homes allows for a more efficient mar- ket and economy. We report the performance of ma- chine learning models trained with structured tabular representations and unstructured text descriptions. We collected a dataset of 200 descriptions of houses which include meta-information, as well as text descriptions. We test logistic regression and multi-layer perceptron (MLP) classifiers on dividing these houses into binary buckets based on fixed price thresholds. We present an exploration into strategies to represent unstructured text descriptions of houses as inputs for machine learning models. This includes a comparison of term frequency- inverse document frequency (TF-IDF), bag-of-words (BoW), and zero-shot inference with large language models. We find the best predictive performance with TF-IDF representations of house descriptions. Readers will gain an understanding of how to use machine learn- ing models optimized with structured and unstructured text data to predict house prices.\n",
      "INFO:root:Narrative text extracted: Introduction Predicting house prices is crucial for buyers, sellers, and the economy as a whole. Real estate prices fluctuate based on multiple factors such as macroeconomic indicators and local economic activity. Agents in the real estate market al- ter their decisions based on their information about the state of the economy. The notion of information asymmetry states that varying information from the buyer or seller could lead to market failure. Spence demonstrated that sellers could signal to buyers in order to avoid adverse selection (Spence 2002). Signaling is the action of one agent with more in- formation conveying insights to the other party. Cypher et al. studied the impact of price signaling from brokers in commercial real estate transactions (Cypher et al. 2017). Their study found that if the price is above the market rate, then pricing is dependent on the strength of the signal. We explore signaling through transparent pricing with accessi- ble predictions from machine learning models. Transparent pricing would lead businesses or individuals to be more con-\n",
      "INFO:root:Narrative text extracted: fident and make decisions quicker, leading to a more effi- cient market. Forecasting house prices is a tool for trans- parent pricing that we explore with machine learning tech- niques.\n",
      "INFO:root:Narrative text extracted: We experiment with predicting house prices with data- driven techniques. The key to data-driven predictive analyt- ics is the feature representation of houses in the dataset. We collect a dataset of prices, meta-information about houses, and written descriptions. Meta-information includes features about houses such as the number of bedrooms, bathrooms, and square feet. Written descriptions are an emerging data source, with excitement fueled by recent advances in deep learning for natural language processing. A written house description is an open-ended report typically written by real estate brokers to describe a particular house.\n",
      "INFO:root:Narrative text extracted: Our experiments present data-driven modeling of 200 houses featurized by meta-information and written descrip- tions. We convert the written descriptions to numeric inputs through the term frequency-inverse document frequency (TF-IDF) algorithm, which is described in further detail in the Related Work section of our report. We begin by pre- senting the performance difference of a logistic regression model mapping either tabular descriptions or TF-IDF vec- tors to binary bins of house prices. The logistic regression model processing tabular data achieves 79.3% training ac- curacy and generalizes to 76.2% test accuracy. The logis- tic regression model with TF-IDF impetus achieves a 2.4% higher test accuracy at 78.6%, however, it has fit the training data very closely, achieving 100% training accuracy. This generalization gap between training and testing inspired our interest into deep learning architectures for TF-IDF repre- sentations of unstructured text data. We utilize a 3-layer non- linear MLP architecture and add dropout to control for over- fitting and limit the variance of the parametric function. We further compare TF-IDF representations of house descrip- tions to BoW representations. We additionally present the zero-shot inference of a 6-billion parameter publicly acces- sible language model to predict house prices.\n",
      "INFO:root:Narrative text extracted: In summary, our contributions are as follows:\n",
      "INFO:root:Narrative text extracted: Copyright © 2022by the authors. All rights reserved.\n",
      "INFO:root:Title extracted: representations of text.\n",
      "INFO:root:Title extracted: Related Work\n",
      "INFO:root:Title extracted: Natural Language Processing and Computer Vision\n",
      "INFO:root:Narrative text extracted: Natural Language Processing (NLP) analyzes and rep- resents human language in order to build statistical mod- els. Hausler et al. conducted an experiment using news- based sentiment on the effects of US securitized and di- rect commercial real estate markets (Hausler et al. 2018). They reported the performance of using a Support Vector Machine (SVM) algorithm to classify the sentiment of real estate news headlines. Velthorst and Guven used text mining and machine learning to predict the Dutch housing market (Velthorst and Guven 2019). The goal of their experiment was to forecast the upward or downward trend of the market based on text data from Twitter. Abdallah and Khashan per- formed text mining to predict the accuracy of home prices by using textual data in real estate classifieds (Abdallah and Khashan 2016).\n",
      "INFO:root:Narrative text extracted: Computer vision is the application of using digital images to train computers to understand the visual world. Recent work has been done to use images listed on property listings to predict house prices. Ahmed and Moustafa developed a dataset that combined photos with textual house information (Ahmed and Moustafa 2016). The combined features were put into a multilayer neural network that predicted the house price. Law et al. developed a deep neural network model that utilizes both visual features from images and traditional tabular features (Law et al. 2019). Their study used street images and satellite image data to improve the estimation of house prices. Bency et al. proposed a Convolutional Neural Network (CNN) framework to model geospatial data to learn the spatial correlations in house prices (Bency et al. 2017).\n",
      "INFO:root:Title extracted: Structured and Unstructured Data Fusion\n",
      "INFO:root:Narrative text extracted: Many other experiments have worked on combining structured and unstructured data. As discussed above, re- searchers have combined digital images with tabular data to model home prices. Cheng et al. experimented with fore- casting financial time-series with multi-modality graph neu- ral networks (Cheng et al. 2021). Cheng et al. proposed us- ing a Multi-Modality Graph Neural Network (MAGNN) to learn from the various inputs (Cheng et al. 2021). Pancerz and Grochowalski explore a mapping algorithm to convert unstructured text data to structured tables (Pancerz and Gro- chowalski 2017).\n",
      "INFO:root:Narrative text extracted: Methodology Our research further explores representations of text data for machine learning modeling. We represent our un- structured descriptions of houses as term frequency-inverse document frequency (TF-IDF) vectors. The TF-IDF algo- rithm begins by computing the number of unique words in the dataset. The representation vectors are initialized with empty vectors. The dimensionality of these empty vectors is sourced from the cardinality of unique words. TF-IDF then populates these vectors by looping through the dataset and mapping the count of each word in its respective data in- stance. The sparse vector is indexed with the token index of each respective word and populated with its frequency. Al- though these representations are very sparse (many 0 entries in the vector), they have been shown to have strong predic- tive power in NLP applications.\n",
      "INFO:root:Narrative text extracted: We further compare TF-IDF representations with Bag-of- Words (BoW) and language modeling with large scale trans- formers. BoW represents a text sequence as a time-series se- quence of categorical labels. Differently from TF-IDF, the input size of BoW is limited with a maximum sequence length parameter, rather than the entire size of the vocab- ulary. For example, bag of words may represent the follow- ing sequence of “The squirrel jumped over the fox” as “[50, 105, 24, 8, 50, 578].” To generalize from BoW to neural em- beddings, each categorical index is converted to a continu- ous vector representation. For example, the “578” categor- ical index used to represent “fox” would be mapped to an n-dimensional continuous vector. These vectors are stacked together to produce a matrix representation of the text data. We experiment with classifying houses into price buck- ets. We begin with a binary classification mapping houses into predictions of greater than or less than $6,000,000. Our dataset contains house descriptions from 4 cities, Boston, Cambridge, Fort Lauderdale, and Boca Raton. Our tabular dataset consists of 5 features: city, street, bedrooms, bath- rooms, and square feet. Our text descriptions contain an av- erage of 200 words with 4,873 uniquely appearing words.\n",
      "INFO:root:Narrative text extracted: The following experiments highlight performance factors of predicting house prices from structured and unstructured data. We begin with a comparison of tabular and TF-IDF data representations. We then show the effectiveness of a deep learning-based Multi-Layer Perceptron (MLP) archi- tecture to model these data sources. Finally, we present a comparison of TF-IDF, BoW, and language modeling repre- sentations of the data and task of house price prediction. Fig- ure 1 illustrates the distribution of labels in our dataset. The prices of our set of houses fall into a power-law distribution. This is an interesting property related to the binary threshold separation which we report in Figure 1 (B). We leave it to fu- ture work to further chunk up these labels into fine-grained categories. For these experiments, we isolate the number of 0 and 1 labels to control for issues due to class imbalance (Johnson and Khoshgoftaar 2019).\n",
      "INFO:root:Title extracted: Tabular versus TF-IDF Representations of Houses\n",
      "INFO:root:Narrative text extracted: We begin our experiments by comparing structured and unstructured data sources for house price classification. We\n",
      "INFO:root:Narrative text extracted: Figure 1: Depicted left, (A) presents the distribution of house prices in our dataset. On the right, (B) illustrates the roughly even distribution after dividing house prices into binary categories based on a price threshold.\n",
      "INFO:root:Narrative text extracted: begin with a logistic regression analysis to ground the ex- periments before introducing a deep MLP architecture. Our tabular data contains 5 features, categorical city and street names, bedrooms, and bathrooms, as well as continuous square feet. Our TF-IDF representation is sourced from a space-separated tokenization of words in the house descrip- tions. We do not include a special out of vocabulary token in our TF-IDF vectorization, such that if a word in the test set is not included in the TF-IDF vectorization formed from the training set, it is not represented during the test inference. Table 1 illustrates the performance of these models. The TF- IDF logistic regression achieves 2.4% higher accuracy than the tabular logistic regression. Further, TF-IDF logistic re- gression is able to perfectly fit the training data and still pos- sess a reasonable generalization ability. This result inspired additional investigation into higher capacity models such as deep MLPs, as well as the use of dropout regularization to reduce the train-test generalization gap.\n",
      "INFO:root:Title extracted: Train Accuracy Test Accuracy\n",
      "INFO:root:Narrative text extracted: Table 1: A comparison of tabular,TF-IDF, and BoW data in- puts, as well as a comparison of logistic regression and MLP models.\n",
      "INFO:root:Title extracted: Deep Learning with Multi-Layer Perceptrons\n",
      "INFO:root:Narrative text extracted: Following experiments with logistic regression models, we explore the effectiveness of a deep MLP model for pre- dicting house prices. Our MLP architecture is adapted for the two input sizes of 4873-d and 5-d for TF-IDF and tab- ular representations, respectively. Both data sources test the same MLP architecture of 3 hidden units containing 512, 1024, and 512 units. Each of these units are passed through a non-linear ReLU activation which prevents negative val- ues in the intermediate activations. Each MLP makes a pre- diction by compressing the last layer of 512 units into a sin- gle prediction modulated with a sigmoid activation function. Due to the differences in input size, the TF-IDF MLP con- tains 5.5 million parameters, whereas the tabular MLP con- tains 1.05 million parameters. Table 1 presents the results of this experiment. The TF-IDF architecture perfectly fits to the training data, however, the logistic regression model per- forms at a 2.4% higher accuracy than the MLP model. In the following section, we turn to dropout regularization to close the performance gap between MLP and logistic regression.\n",
      "INFO:root:Title extracted: The Impact of Dropout on MLP Predictions\n",
      "INFO:root:Narrative text extracted: perfect 100% training accuracy. However, this failed to gen- eralize to the held-out test set, only achieving 76.2% test accuracy. Table 2 illustrates the benefit of applying dropout regularization to the MLP model. Dropout describes ran- domly zeroing out either inputs or intermediate activations, depending on where in the neural network architecture the dropout layer is placed. Dropout is used to regularize deep learning models and avoid overfitting such as what we have observed with 100% training accuracy and 76.2% testing ac- curacy. We utilize a high level of dropout at 75% placed at the input and in between every hidden layer of our 3-layer MLP network. This results in an increase in the test accuracy from 76.2% to 78.6% when processing TF-IDF data rep- resentations. Dropout similarly increases the tabular MLP from 71.4% to 73.8% test accuracy. Although dropout de- creases the train-test generalization gap, we still have 100% training accuracy. We think this illustrates an opportunity for future improvement by exploring additional regulariza- tions for deep neural networks, such as Data Augmentation (Shorten et al. 2021, Shorten and Khoshgoftaar 2019).\n",
      "INFO:root:Narrative text extracted: Our initial experiment applying a Deep MLP architecture to TF-IDF representations of house descriptions achieved a\n",
      "INFO:root:Title extracted: TF-IDF versus Bag-of-Words\n",
      "INFO:root:Narrative text extracted: In addition to TF-IDF representations of house descrip- tions, we also tested Bag-of-Words (BoW) representations. BoW describes representing text inputs as a sequence of to- ken indexes. This limits the length of the input sequences compared to TF-IDF in which the input is the size of the entire vocabulary and each index is populated by the term frequency in that particular instance. We find better per- formance representing text as TF-IDF vectors, compared to BoW vectors. When training an MLP on BoW vectors without dropout it quickly overfits the training set achieving 100% training accuracy and a maximum entropy 50% test accuracy. Table 1 reports an MLP with dropout layers placed between every layer for both BoW and TF-IDF inputs.\n",
      "INFO:root:Title extracted: Discussion\n",
      "INFO:root:Title extracted: Multimodal Data Fusion\n",
      "INFO:root:Narrative text extracted: These experiments highlight the unimodal performance of modeling tabular and text data sources. In future work, we aim to test a fusion of models processing both tabular and text data sources. A simple way to extend our work to multimodal structured and unstructured prediction would be to have a linear weighting between each model prediction, such as alpha * tabular prediction + (1 - alpha) * text predic- tion. This describes a simple ensembling technique where we treat each model as a black box prediction. Another ap- proach we intend to explore in future work is to combine the intermediate representations of each data type in our MLP architecture. This is known as intermediate fusion in the lit- erature on multimodal architecture design for deep learning.\n",
      "INFO:root:Title extracted: Data Splitting for Generalization Tests\n",
      "INFO:root:Narrative text extracted: In future work, we additionally intend to test the proce- dure in which we evaluate the generalization of our house price prediction models. These experiments use the inde- pendent and identically distributed (i.i.d.) procedure of ran- domly sampling train and test instances from a pool of data drawn from the same underlying distribution. In fu- ture work, we intend to isolate domains in which the data is sourced from. For example, training on house descriptions in Boston and Cambridge and evaluating on Boca Raton and Fort Lauderdale. We believe these kinds of train-test splits will provide more insight as to how well these models per- form. For example, text descriptions of houses in Florida are likely to be very different from houses in Alaska.\n",
      "INFO:root:Narrative text extracted: Conclusion Transparent pricing in the real estate market will lead to a more efficient economy. It allows agents to make informed decisions about trading decisions. Our experiment shows the impact of using structured and unstructured data for house price predictions. The final results conveyed that TF-IDF representations of text data perform better than tabular fea- tures. We believe this is just scratching the surface of natu- ral language processing techniques to process this text data. Further, we intend to explore combining structured and un- structured data sources for house price prediction.\n",
      "INFO:root:Narrative text extracted: Acknowledgments We acknowledge partial support by the NSF NRT-HDR (2021585). Opinions, findings, conclusions, or recommen- dations in this paper are the authors’ and do not reflect the views of the NSF.\n",
      "INFO:root:Narrative text extracted: References Abdallah, S., Khashan, D. A. 2016. Using Text Mining to Analyze Real Estate Classifieds. In International Conference on Advanced Intelligent Systems and Informatics. Ahmed, E. H., Moustafa, M. N. 2016. House Price Estima- tion from Visual and Textual Features. ArXiv:1609.08399. In NCTA, 8th International Conference on Neural Compu- tation Theory and Applications. Bency, A., Rallapalli, S., Ganti, R. K., Srivatsa, M., Manju- nath, B. S. 2017. Beyond Spatial Auto-Regressive Models: Predicting Housing Prices with Satellite Imagery. In 2017 IEEE Winter Conference on Applications of Computer Vi- sion (WACV), 2017, pp. 320-329. Cheng, B., Yang, F., Xiang, S., Liu, J. 2021. Financial time series forecasting with tern Recognition, Volume 121, 2022. Cypher, M., Price, S. M., Robinson, S., Seiler, M. 2017. Price Signals and Uncertainty in Commercial Real Estate Transactions. In Journal of Real Estate Finance and Eco- nomics, Forthcoming. Gao L., Biderman S., Black S., Golding L., et al. 2020. The Pile: An 800GB Dataset of Diverse Text for Language Mod- eling. In arXiv:2101.00027. Hausler, J., Ruscheinsky, J., Lang, M. 2018. News-Based Sentiment Analysis in Real Estate: a Machine Learning Ap- proach. In Journal of Property Research Volume 35, Issue 4, pages 344-371. Johnson J. M., Khoshgoftaar T. M. 2019. Survey on deep learning with class imbalance. In Journal of Big Data, vol- ume 6, Article number 27. Law, S., Paige, B., Russell, C. 2019. Take a Look Around: Using Street View and Satellite Images to Estimate House Prices. ArXiv: 1807.07155. In ACM Transactions on Intel- ligent Systems and Technology 10(5):1-19. Pancerz, K., Grochowalski, P. 2017. From unstructured data included in real-estate listings to information systems over ontological graphs. In International Conference on Informa- tion and Digital Technologies (IDT), 2017, pp. 298-303. Spence, A. M. 2002. Signaling in Retrospect and the Infor- mational Structure of Markets. In American Economic Re- view, Vol. 92, No. 3, pages 434-459. Shorten, C., Khoshgoftaar M. T., Furht B. 2021. Text Data Augmentation for Deep Learning. In Journal of Big Data. Shorten, C., Khoshgoftaar M. T. A survey on Image Data Augmentation for Deep Learning. In Journal of Big Data. Velthorst, M., Guven, C. 2019. Predicting Housing Market Trends Using Twitter Data. In 2019 6th Swiss Conference on Data Science (SDS).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"error\": [\n",
      "    {\n",
      "      \"message\": \"invalid object: invalid UUID length: 0\"\n",
      "    }\n",
      "  ],\n",
      "  \"valid\": false\n",
      "}\n",
      "{'error': [{'message': \"invalid text property 'title' on class 'Test': not a string, but []interface {}\"}]}\n",
      "Processing paper01.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Title extracted: SURVEY PAPER\n",
      "INFO:root:Title extracted: Open Access\n",
      "INFO:root:Title extracted: A survey on Image Data Augmentation for Deep Learning\n",
      "INFO:root:Title extracted: Connor Shorten*\n",
      "INFO:root:Title extracted: and Taghi M. Khoshgoftaar\n",
      "INFO:root:Narrative text extracted: Abstract Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfor- tunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The applica- tion of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other character- istics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.\n",
      "INFO:root:Title extracted: Keywords: Data Augmentation, Big data, Image data, Deep Learning, GANs\n",
      "INFO:root:Title extracted: Introduction\n",
      "INFO:root:Narrative text extracted: Deep Learning models have made incredible progress in discriminative tasks. This has been fueled by the advancement of deep network architectures, powerful computation, and access to big data. Deep neural networks have been successfully applied to Com- puter Vision tasks such as image classification, object detection, and image segmenta- tion thanks to the development of convolutional neural networks (CNNs). These neural networks utilize parameterized, sparsely connected kernels which preserve the spatial characteristics of images. Convolutional layers sequentially downsample the spatial resolution of images while expanding the depth of their feature maps. This series of convolutional transformations can create much lower-dimensional and more useful rep- resentations of images than what could possibly be hand-crafted. The success of CNNs has spiked interest and optimism in applying Deep Learning to Computer Vision tasks.\n",
      "INFO:root:Narrative text extracted: © The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.\n",
      "INFO:root:Narrative text extracted: Fig. 1 The plot on the left shows an inflection point where the validation error starts to increase as the training rate continues to decrease. The increased training has caused the model to overfit to the training data and perform poorly on the testing set relative to the training set. In contrast, the plot on the right shows a model with the desired relationship between training and testing error\n",
      "INFO:root:Narrative text extracted: There are many branches of study that hope to improve current benchmarks by apply- ing deep convolutional networks to Computer Vision tasks. Improving the generaliza- tion ability of these models is one of the most difficult challenges. Generalizability refers to the performance difference of a model when evaluated on previously seen data (train- ing data) versus data it has never seen before (testing data). Models with poor general- izability have overfitted the training data. One way to discover overfitting is to plot the training and validation accuracy at each epoch during training. The graph below depicts what overfitting might look like when visualizing these accuracies over training epochs (Fig. 1).\n",
      "INFO:root:Narrative text extracted: To build useful Deep Learning models, the validation error must continue to decrease with the training error. Data Augmentation is a very powerful method of achieving this. The augmented data will represent a more comprehensive set of possible data points, thus minimizing the distance between the training and validation set, as well as any future testing sets.\n",
      "INFO:root:Narrative text extracted: Data Augmentation, the focus of this survey, is not the only technique that has been developed to reduce overfitting. The following few paragraphs will introduce other solu- tions available to avoid overfitting in Deep Learning models. This listing is intended to give readers a broader understanding of the context of Data Augmentation.\n",
      "INFO:root:Narrative text extracted: Many other strategies for increasing generalization performance focus on the model’s architecture itself. This has led to a sequence of progressively more complex architec- tures from AlexNet [1] to VGG-16 [2], ResNet [3], Inception-V3 [4], and DenseNet [5]. Functional solutions such as dropout regularization, batch normalization, transfer learn- ing, and pretraining have been developed to try to extend Deep Learning for application on smaller datasets. A brief description of these overfitting solutions is provided below. A complete survey of regularization methods in Deep Learning has been compiled by Kukacka et al. [6]. Knowledge of these overfitting solutions will inform readers about other existing tools, thus framing the high-level context of Data Augmentation and Deep Learning.\n",
      "INFO:root:Narrative text extracted: networks with Spatial Dropout, which drops out entire feature maps rather than individual neurons.\n",
      "INFO:root:Narrative text extracted: In contrast to the techniques mentioned above, Data Augmentation approaches overfitting from the root of the problem, the training dataset. This is done under the assumption that more information can be extracted from the original dataset through augmentations. These augmentations artificially inflate the training dataset size by either data warping or oversampling. Data warping augmentations transform exist- ing images such that their label is preserved. This encompasses augmentations such as geometric and color transformations, random erasing, adversarial training, and neural style transfer. Oversampling augmentations create synthetic instances and add them to the training set. This includes mixing images, feature space augmentations, and genera- tive adversarial networks (GANs). Oversampling and Data Warping augmentations do not form a mutually exclusive dichotomy. For example, GAN samples can be stacked with random cropping to further inflate the dataset. Decisions around final dataset size,\n",
      "INFO:root:Narrative text extracted: Fig. 2 A taxonomy of image data augmentations covered; the colored lines in the figure depict which data augmentation method the corresponding meta-learning scheme uses, for example, meta-learning using Neural Style Transfer is covered in neural augmentation [36]\n",
      "INFO:root:Narrative text extracted: test-time augmentation, curriculum learning, and the impact of resolution are covered in this survey under the “Design considerations for image Data Augmentation” section. Descriptions of individual augmentation techniques will be enumerated in the “Image Data Augmentation techniques” section. A quick taxonomy of the Data Augmentations is depicted below in Fig. 2.\n",
      "INFO:root:Narrative text extracted: Before discussing image augmentation techniques, it is useful to frame the context of the problem and consider what makes image recognition such a difficult task in the first place. In classic discriminative examples such as cat versus dog, the image recognition software must overcome issues of viewpoint, lighting, occlusion, background, scale, and more. The task of Data Augmentation is to bake these translational invariances into the dataset such that the resulting models will perform well despite these challenges.\n",
      "INFO:root:Narrative text extracted: It is a generally accepted notion that bigger datasets result in better Deep Learning models [23, 24]. However, assembling enormous datasets can be a very daunting task due to the manual effort of collecting and labeling data. Limited datasets is an especially prevalent challenge in medical image analysis. Given big data, deep convolutional net- works have been shown to be very powerful for medical image analysis tasks such as skin lesion classification as demonstrated by Esteva et al. [25]. This has inspired the use of CNNs on medical image analysis tasks [26] such as liver lesion classification, brain scan analysis, continued research in skin lesion classification, and more. Many of the images studied are derived from computerized tomography (CT) and magnetic resonance imag- ing (MRI) scans, both of which are expensive and labor-intensive to collect. It is espe- cially difficult to build big medical image datasets due to the rarity of diseases, patient\n",
      "INFO:root:Narrative text extracted: privacy, the requirement of medical experts for labeling, and the expense and manual effort needed to conduct medical imaging processes. These obstacles have led to many studies on image Data Augmentation, especially GAN-based oversampling, from the application perspective of medical image classification.\n",
      "INFO:root:Narrative text extracted: Many studies on the effectiveness of Data Augmentation utilize popular academic image datasets to benchmark results. These datasets include MNIST hand written digit recognition, CIFAR-10/100, ImageNet, tiny-imagenet-200, SVHN (street view house numbers), Caltech-101/256, MIT places, MIT-Adobe 5K dataset, Pascal VOC, and Stan- ford Cars. The datasets most frequently discussed are CIFAR-10, CIFAR-100, and Ima- geNet. The expansion of open-source datasets has given researchers a wide variety of cases to compare performance results of Data Augmentation techniques. Most of these datasets such as ImageNet would be classified as big data. Many experiments constrain themselves to a subset of the dataset to simulate limited data problems.\n",
      "INFO:root:Narrative text extracted: In addition to our focus on limited datasets, we will also consider the problem of class imbalance and how Data Augmentation can be a useful oversampling solution. Class imbalance describes a dataset with a skewed ratio of majority to minority samples. Leevy et al. [27] describe many of the existing solutions to high-class imbalance across data types. Our survey will show how class-balancing oversampling in image data can be done with Data Augmentation.\n",
      "INFO:root:Narrative text extracted: Many aspects of Deep Learning and neural network models draw comparisons with human intelligence. For example, a human intelligence anecdote of transfer learning is illustrated in learning music. If two people are trying to learn how to play the guitar, and one already knows how to play the piano, it seems likely that the piano-player will learn to play the guitar faster. Analogous to learning music, a model that can classify Ima- geNet images will likely perform better on CIFAR-10 images than a model with random weights.\n",
      "INFO:root:Narrative text extracted: Data Augmentation is similar to imagination or dreaming. Humans imagine differ- ent scenarios based on experience. Imagination helps us gain a better understanding of our world. Data Augmentation methods such as GANs and Neural Style Transfer can ‘imagine’ alterations to images such that they have a better understanding of them. The remainder of the paper is organized as follows: A brief “Background” is provided to give readers a historical context of Data Augmentation and Deep Learning. “Image Data Augmentation techniques” discusses each image augmentation technique in detail along with experimental results. “Design considerations for image Data Augmentation” discusses additional characteristics of augmentation such as test-time augmentation and the impact of image resolution. The paper concludes with a “Discussion” of the pre- sented material, areas of “Future work”, and “Conclusion”.\n",
      "INFO:root:Title extracted: Background\n",
      "INFO:root:Narrative text extracted: Image augmentation in the form of data warping can be found in LeNet-5 [28]. This was one of the first applications of CNNs on handwritten digit classification. Data augmentation has also been investigated in oversampling applications. Oversampling is a technique used to re-sample imbalanced class distributions such that the model is not overly biased towards labeling instances as the majority class type. Random Oversampling (ROS) is a naive approach which duplicates images randomly from the\n",
      "INFO:root:Narrative text extracted: minority class until a desired class ratio is achieved. Intelligent oversampling tech- niques date back to SMOTE (Synthetic Minority Over-sampling Technique), which was developed by Chawla et al. [29]. SMOTE and the extension of Borderline-SMOTE [30] create new instances by interpolating new points from existing instances via k-Nearest Neighbors. The primary focus of this technique was to alleviate problems due to class imbalance, and SMOTE was primarily used for tabular and vector data.\n",
      "INFO:root:Narrative text extracted: The AlexNet CNN architecture developed by Krizhevsky et al. [1] revolutionized image classification by applying convolutional networks to the ImageNet dataset. Data Augmentation is used in their experiments to increase the dataset size by a magnitude of 2048. This is done by randomly cropping 224 × 224 patches from the original images, flipping them horizontally, and changing the intensity of the RGB channels using PCA color augmentation. This Data Augmentation helped reduce overfitting when training a deep neural network. The authors claim that their aug- mentations reduced the error rate of the model by over 1%.\n",
      "INFO:root:Narrative text extracted: Since then, GANs were introduced in 2014 [31], Neural Style Transfer [32] in 2015, and Neural Architecture Search (NAS) [33] in 2017. Various works on GAN exten- sions such as DCGANs, CycleGANs and Progressively-Growing GANs [34] were pub- lished in 2015, 2017, and 2017, respectively. Neural Style Transfer was sped up with the development of Perceptual Losses by Johnson et al. [35] in 2016. Applying meta- learning concepts from NAS to Data Augmentation has become increasingly popular with works such as Neural Augmentation [36], Smart Augmentation [37], and Auto- Augment [38] published in 2017, 2017, and 2018, respectively.\n",
      "INFO:root:Narrative text extracted: Applying Deep Learning to medical imaging has been a popular application for CNNs since they became so popular in 2012. Deep Learning and medical imaging became increasingly popular with the demonstration of dermatologist-level skin can- cer detection by Esteva et al. [25] in 2017.\n",
      "INFO:root:Narrative text extracted: The use of GANs in medical imaging is well documented in a survey by Yi et al. [39]. This survey covers the use of GANs in reconstruction such as CT denoising [40], accelerated magnetic resonance imaging [41], PET denoising [42], and the applica- tion of super-resolution GANs in retinal vasculature segmentation [43]. Additionally, Yi et al. [39] cover the use of GAN image synthesis in medical imaging applications such as brain MRI synthesis [44, 45], lung cancer diagnosis [46], high-resolution skin lesion synthesis [47], and chest x-ray abnormality classification [48]. GAN-based image synthesis Data Augmentation was used by Frid-Adar et al. [49] in 2018 for liver lesion classification. This improved classification performance from 78.6% sensitiv- ity and 88.4% specificity using classic augmentations to 85.7% sensitivity and 92.4% specificity using GAN-based Data Augmentation.\n",
      "INFO:root:Narrative text extracted: Most of the augmentations covered focus on improving Image Recognition mod- els. Image Recognition is when a model predicts an output label such as ‘dog’ or ‘cat’ given an input image.\n",
      "INFO:root:Narrative text extracted: However, it is possible to extend results from image recognition to other Computer Vision tasks such as Object Detection led by the algorithms YOLO [50], R-CNN [51], fast R-CNN [52], and faster R-CNN [53] or Semantic Segmentation [54] including algorithms such as U-Net [55].\n",
      "INFO:root:Title extracted: Image Data Augmentation techniques\n",
      "INFO:root:Narrative text extracted: The earliest demonstrations showing the effectiveness of Data Augmentations come from simple transformations such as horizontal flipping, color space augmentations, and random cropping. These transformations encode many of the invariances discussed ear- lier that present challenges to image recognition tasks. The augmentations listed in this survey are geometric transformations, color space transformations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, GAN-based augmentation, neural style transfer, and meta-learning schemes. This section will explain how each augmentation algorithm works, report experimental results, and discuss dis- advantages of the augmentation technique.\n",
      "INFO:root:Narrative text extracted: Data Augmentations based on basic image manipulations\n",
      "INFO:root:Title extracted: Geometric transformations\n",
      "INFO:root:Narrative text extracted: This section describes different augmentations based on geometric transformations and many other image processing functions. The class of augmentations discussed below could be characterized by their ease of implementation. Understanding these trans- formations will provide a useful base for further investigation into Data Augmentation techniques.\n",
      "INFO:root:Narrative text extracted: We will also describe the different geometric augmentations in the context of their ‘safety’ of application. The safety of a Data Augmentation method refers to its likelihood of preserving the label post-transformation. For example, rotations and flips are gener- ally safe on ImageNet challenges such as cat versus dog, but not safe for digit recogni- tion tasks such as 6 versus 9. A non-label preserving transformation could potentially strengthen the model’s ability to output a response indicating that it is not confident about its prediction. However, achieving this would require refined labels [56] post-aug- mentation. If the label of the image after a non-label preserving transformation is some- thing like [0.5 0.5], the model could learn more robust confidence predictions. However, constructing refined labels for every non-safe Data Augmentation is a computationally expensive process.\n",
      "INFO:root:Narrative text extracted: Due to the challenge of constructing refined labels for post-augmented data, it is important to consider the ‘safety’ of an augmentation. This is somewhat domain depend- ent, providing a challenge for developing generalizable augmentation policies, (see Auto- Augment [38] for further exploration into finding generalizable augmentations). There is no image processing function that cannot result in a label changing transformation at some distortion magnitude. This demonstrates the data-specific design of augmen- tations and the challenge of developing generalizable augmentation policies. This is an important consideration with respect to the geometric augmentations listed below.\n",
      "INFO:root:Title extracted: Flipping\n",
      "INFO:root:Narrative text extracted: Horizontal axis flipping is much more common than flipping the vertical axis. This aug- mentation is one of the easiest to implement and has proven useful on datasets such as CIFAR-10 and ImageNet. On datasets involving text recognition such as MNIST or SVHN, this is not a label-preserving transformation.\n",
      "INFO:root:Title extracted: Color space\n",
      "INFO:root:Narrative text extracted: Digital image data is usually encoded as a tensor of the dimension (height × width × color channels). Performing augmentations in the color channels space is another strategy that is very practical to implement. Very simple color augmentations include isolating a single color channel such as R, G, or B. An image can be quickly converted into its representation in one color channel by isolating that matrix and adding 2 zero matrices from the other color channels. Additionally, the RGB values can be easily manipulated with simple matrix operations to increase or decrease the brightness of the image. More advanced color augmentations come from deriving a color histogram describing the image. Changing the intensity values in these histograms results in lighting alterations such as what is used in photo editing applications.\n",
      "INFO:root:Title extracted: Cropping\n",
      "INFO:root:Narrative text extracted: Cropping images can be used as a practical processing step for image data with mixed height and width dimensions by cropping a central patch of each image. Additionally, random cropping can also be used to provide an effect very similar to translations. The contrast between random cropping and translations is that cropping will reduce the size of the input such as (256,256) → (224, 224), whereas translations preserve the spatial dimensions of the image. Depending on the reduction threshold chosen for cropping, this might not be a label-preserving transformation.\n",
      "INFO:root:Title extracted: Rotation\n",
      "INFO:root:Narrative text extracted: Rotation augmentations are done by rotating the image right or left on an axis between 1° and 359°. The safety of rotation augmentations is heavily determined by the rotation degree parameter. Slight rotations such as between 1 and 20 or − 1 to − 20 could be useful on digit recognition tasks such as MNIST, but as the rotation degree increases, the label of the data is no longer preserved post-transformation.\n",
      "INFO:root:Title extracted: Translation\n",
      "INFO:root:Narrative text extracted: Shifting images left, right, up, or down can be a very useful transformation to avoid positional bias in the data. For example, if all the images in a dataset are centered, which is common in face recognition datasets, this would require the model to be tested on perfectly centered images as well. As the original image is translated in a direction, the remaining space can be filled with either a constant value such as 0 s or 255 s, or it can be filled with random or Gaussian noise. This padding preserves the spatial dimensions of the image post-augmentation.\n",
      "INFO:root:Title extracted: Noise injection\n",
      "INFO:root:Narrative text extracted: Noise injection consists of injecting a matrix of random values usually drawn from a Gaussian distribution. Noise injection is tested by Moreno-Barea et al. [57] on nine datasets from the UCI repository [58]. Adding noise to images can help CNNs learn more robust features.\n",
      "INFO:root:Narrative text extracted: Geometric transformations are very good solutions for positional biases present in the training data. There are many potential sources of bias that could separate the\n",
      "INFO:root:Narrative text extracted: Fig. 3 Examples of Color Augmentations provided by Mikolajczyk and Grochowski [72] in the domain of melanoma classification\n",
      "INFO:root:Narrative text extracted: distribution of the training data from the testing data. If positional biases are pre- sent, such as in a facial recognition dataset where every face is perfectly centered in the frame, geometric transformations are a great solution. In addition to their pow- erful ability to overcome positional biases, geometric transformations are also use- ful because they are easily implemented. There are many imaging processing libraries that make operations such as horizontal flipping and rotation painless to get started with. Some of the disadvantages of geometric transformations include additional memory, transformation compute costs, and additional training time. Some geo- metric transformations such as translation or random cropping must be manually observed to make sure they have not altered the label of the image. Finally, in many of the application domains covered such as medical image analysis, the biases distancing the training data from the testing data are more complex than positional and transla- tional variances. Therefore, the scope of where and when geometric transformations can be applied is relatively limited.\n",
      "INFO:root:Title extracted: Color space transformations\n",
      "INFO:root:Narrative text extracted: Image data is encoded into 3 stacked matrices, each of size height × width. These matri- ces represent pixel values for an individual RGB color value. Lighting biases are amongst the most frequently occurring challenges to image recognition problems. Therefore, the effectiveness of color space transformations, also known as photometric transforma- tions, is fairly intuitive to conceptualize. A quick fix to overly bright or dark images is to loop through the images and decrease or increase the pixel values by a constant value. Another quick color space manipulation is to splice out individual RGB color matrices. Another transformation consists of restricting pixel values to a certain min or max value. The intrinsic representation of color in digital images lends itself to many strategies of augmentation.\n",
      "INFO:root:Narrative text extracted: Color space transformations can also be derived from image-editing apps. An image’s pixel values in each RGB color channel is aggregated to form a color histogram. This his- togram can be manipulated to apply filters that change the color space characteristics of an image.\n",
      "INFO:root:Narrative text extracted: There is a lot of freedom for creativity with color space augmentations. Altering the color distribution of images can be a great solution to lighting challenges faced by testing data (Figs. 3, 4).\n",
      "INFO:root:Narrative text extracted: Image datasets can be simplified in representation by converting the RGB matri- ces into a single grayscale image. This results in smaller images, height × width × 1,\n",
      "INFO:root:Narrative text extracted: Fig. 4 Examples of color augmentations tested by Wu et al. [127]\n",
      "INFO:root:Narrative text extracted: resulting in faster computation. However, this has been shown to reduce per- formance accuracy. Chatifled et al. [59] found a ~ 3% classification accuracy drop between grayscale and RGB images with their experiments on ImageNet [12] and the PASCAL [60] VOC dataset. In addition to RGB versus grayscale images, there are many other ways of representing digital color such as HSV (Hue, Saturation, and Value). Jurio et al. [61] explore the performance of Image Segmentation on many dif- ferent color space representations from RGB to YUV, CMY, and HSV.\n",
      "INFO:root:Narrative text extracted: Similar to geometric transformations, a disadvantage of color space transforma- tions is increased memory, transformation costs, and training time. Additionally, color transformations may discard important color information and thus are not always a label-preserving transformation. For example, when decreasing the pixel values of an image to simulate a darker environment, it may become impossible to see the objects in the image. Another indirect example of non-label preserving color transformations is in Image Sentiment Analysis [62]. In this application, CNNs try to visually predict the sentiment score of an image such as: highly negative, nega- tive, neutral, positive, or highly positive. One indicator of a negative/highly negative image is the presence of blood. The dark red color of blood is a key component to distinguish blood from water or paint. If color space transforms repeatedly change the color space such that the model cannot recognize red blood from green paint, the model will perform poorly on Image Sentiment Analysis. In effect, color space transformations will eliminate color biases present in the dataset in favor of spa- tial characteristics. However, for some tasks, color is a very important distinctive feature.\n",
      "INFO:root:Title extracted: Top-1 accuracy (%)\n",
      "INFO:root:Title extracted: Top-5 accuracy (%)\n",
      "INFO:root:Title extracted: Baseline\n",
      "INFO:root:Title extracted: Flipping\n",
      "INFO:root:Title extracted: Rotating\n",
      "INFO:root:Title extracted: Cropping\n",
      "INFO:root:Title extracted: Color Jittering\n",
      "INFO:root:Title extracted: Edge Enhancement\n",
      "INFO:root:Title extracted: Fancy PCA\n",
      "INFO:root:Narrative text extracted: Their results find that the cropping geometric transformation results in the most accurate classifier\n",
      "INFO:root:Narrative text extracted: The italic value denote high performance according to the comparative metrics\n",
      "INFO:root:Title extracted: Geometric versus photometric transformations\n",
      "INFO:root:Narrative text extracted: Taylor and Nitschke [63] provide a comparative study on the effectiveness of geometric and photometric (color space) transformations. The geometric transformations stud- ied were flipping, − 30° to 30° rotations, and cropping. The color space transformations studied were color jittering, (random color manipulation), edge enhancement, and PCA. They tested these augmentations with 4-fold cross-validation on the Caltech101 dataset filtered to 8421 images of size 256 × 256 (Table 1).\n",
      "INFO:root:Title extracted: Kernel filters\n",
      "INFO:root:Narrative text extracted: Kernel filters are a very popular technique in image processing to sharpen and blur images. These filters work by sliding an n × n matrix across an image with either a Gauss- ian blur filter, which will result in a blurrier image, or a high contrast vertical or hori- zontal edge filter which will result in a sharper image along edges. Intuitively, blurring images for Data Augmentation could lead to higher resistance to motion blur during testing. Additionally, sharpening images for Data Augmentation could result in encapsu- lating more details about objects of interest.\n",
      "INFO:root:Narrative text extracted: Sharpening and blurring are some of the classical ways of applying kernel filters to images. Kang et al. [64] experiment with a unique kernel filter that randomly swaps the pixel values in an n × n sliding window. They call this augmentation technique PatchShuffle Regularization. Experimenting across different filter sizes and probabilities of shuffling the pixels at each step, they demonstrate the effectiveness of this by achiev- ing a 5.66% error rate on CIFAR-10 compared to an error rate of 6.33% achieved with- out the use of PatchShuffle Regularization. The hyperparameter settings that achieved this consisted of 2 × 2 filters and a 0.05 probability of swapping. These experiments were done using the ResNet [3] CNN architecture (Figs. 5, 6).\n",
      "INFO:root:Narrative text extracted: Kernel filters are a relatively unexplored area for Data Augmentation. A disadvantage of this technique is that it is very similar to the internal mechanisms of CNNs. CNNs have parametric kernels that learn the optimal way to represent images layer-by-layer. For example, something like PatchShuffle Regularization could be implemented with a convolution layer. This could be achieved by modifying the standard convolution layer parameters such that the padding parameters preserve spatial resolution and the sub- sequent activation layer keeps pixel values between 0 and 255, in contrast to something like a sigmoid activation which maps pixels to values between 0 and 1. Therefore kernel\n",
      "INFO:root:Narrative text extracted: Fig. 5 Examples of applying the PatchShuffle regularization technique [64]\n",
      "INFO:root:Title extracted: Fig. 6 Pixels in a n\n",
      "INFO:root:Narrative text extracted: n window are randomly shifted with a probability parameter p\n",
      "INFO:root:Narrative text extracted: filters can be better implemented as a layer of the network rather than as an addition to the dataset through Data Augmentation.\n",
      "INFO:root:Narrative text extracted: Mixing images\n",
      "INFO:root:Narrative text extracted: Mixing images together by averaging their pixel values is a very counterintuitive approach to Data Augmentation. The images produced by doing this will not look like a useful transformation to a human observer. However, Ionue [65] demonstrated how the pairing of samples could be developed into an effective augmentation strategy. In this experiment, two images are randomly cropped from 256 × 256 to 224 × 224 and ran- domly flipped horizontally. These images are then mixed by averaging the pixel values for each of the RGB channels. This results in a mixed image which is used to train a clas- sification model. The label assigned to the new image is the same as the first randomly selected image (Fig. 7).\n",
      "INFO:root:Narrative text extracted: On the CIFAR-10 dataset, Ionue reported a reduction in error rate from 8.22 to 6.93% when using the SamplePairing Data Augmentation technique. The researcher found even better results when testing a reduced size dataset, reducing CIFAR-10 to 1000 total samples with 100 in each class. With the reduced size dataset, SamplePairing resulted in an error rate reduction from 43.1 to 31.0%. The reduced CIFAR-10 results demonstrate the usefulness of the SamplePairing technique in limited data applications (Fig. 8).\n",
      "INFO:root:Narrative text extracted: Another detail found in the study is that better results were obtained when mixing images from the entire training set rather than from instances exclusively belonging to the same class. Starting from a training set of size N, SamplePairing produces a dataset of size N2 + N. In addition, Sample Pairing can be stacked on top of other augmentation techniques. For example, if using the augmentations demonstrated in\n",
      "INFO:root:Narrative text extracted: Fig. 7 SamplePairing augmentation strategy [65]\n",
      "INFO:root:Narrative text extracted: Fig. 8 Results on the reduced CIFAR-10 dataset. Experimental results demonstrated with respect to sampling pools for image mixing [65]\n",
      "INFO:root:Narrative text extracted: the AlexNet paper by Krizhevsky et al. [1], the 2048 × dataset increase can be further expanded to (2048 × N)2. The concept of mixing images in an unintuitive way was further investigated by Summers and Dinneen [66]. They looked at using non-linear methods to combine images into new training instances. All of the methods they used resulted in better performance compared to the baseline models (Fig. 9).\n",
      "INFO:root:Narrative text extracted: Amongst these non-linear augmentations tested, the best technique resulted in a reduction from 5.4 to 3.8% error on CIFAR-10 and 23.6% to 19.7% on CIFAR-100. In like manner, Liang et al. [67] used GANs to produce mixed images. They found that the inclusion of mixed images in the training data reduced training time and increased the diversity of GAN-samples. Takahashi and Matsubara [68] experiment\n",
      "INFO:root:Narrative text extracted: Fig. 9 Non-linearly mixing images [66]\n",
      "INFO:root:Narrative text extracted: Fig. 10 Mixing images through random image cropping and patching [68]\n",
      "INFO:root:Narrative text extracted: with another approach to mixing images that randomly crops images and concate- nates the croppings together to form new images as depicted below. The results of their technique, as well as SamplePairing and mixup augmentation, demonstrate the sometimes unreasonable effectiveness of big data with Deep Learning models (Fig. 10).\n",
      "INFO:root:Narrative text extracted: An obvious disadvantage of this technique is that it makes little sense from a human perspective. The performance boost found from mixing images is very difficult to understand or explain. One possible explanation for this is that the increased dataset size results in more robust representations of low-level characteristics such as lines and edges. Testing the performance of this in comparisons to transfer learning and pretrain- ing methods is an interesting area for future work. Transfer learning and pretraining are other techniques that learn low-level characteristics in CNNs. Additionally, it will be\n",
      "INFO:root:Narrative text extracted: Fig. 11 Example of random erasing on image recognition tasks [70]\n",
      "INFO:root:Narrative text extracted: interesting to see how the performance changes if we partition the training data such that the first 100 epochs are trained with original and mixed images and the last 50 with original images only. These kinds of strategies are discussed further in Design Consid- erations of Data Augmentation with respect to curriculum learning [69]. Additionally, the paper will cover a meta-learning technique developed by Lemley et al. [37] that uses a neural network to learn an optimal mixing of images.\n",
      "INFO:root:Narrative text extracted: Random erasing\n",
      "INFO:root:Narrative text extracted: Random erasing [70] is another interesting Data Augmentation technique developed by Zhong et al. Inspired by the mechanisms of dropout regularization, random erasing can be seen as analogous to dropout except in the input data space rather than embed- ded into the network architecture. This technique was specifically designed to combat image recognition challenges due to occlusion. Occlusion refers to when some parts of the object are unclear. Random erasing will stop this by forcing the model to learn more descriptive features about an image, preventing it from overfitting to a certain visual fea- ture in the image. Aside from the visual challenge of occlusion, in particular, random erasing is a promising technique to guarantee a network pays attention to the entire image, rather than just a subset of it.\n",
      "INFO:root:Narrative text extracted: Random erasing works by randomly selecting an n × m patch of an image and masking it with either 0 s, 255 s, mean pixel values, or random values. On the CIFAR-10 dataset this resulted in an error rate reduction from 5.17 to 4.31%. The best patch fill method was found to be random values. The fill method and size of the masks are the only parameters that need to be hand-designed during implementation (Figs. 11, 12).\n",
      "INFO:root:Narrative text extracted: Random erasing is a Data Augmentation method that seeks to directly prevent overfit- ting by altering the input space. By removing certain input patches, the model is forced to find other descriptive characteristics. This augmentation method can also be stacked on top of other augmentation techniques such as horizontal flipping or color filters. Ran- dom erasing produced one of the highest accuracies on the CIFAR-10 dataset. DeVries and Taylor [71] conducted a similar study called Cutout Regularization. Like the random erasing study, they experimented with randomly masking regions of the image (Table 2). Mikolajcyzk and Grochowski [72] presented an interesting idea to combine random erasing with GANs designed for image inpainting. Image inpainting describes the task of\n",
      "INFO:root:Narrative text extracted: Fig. 12 Example of random erasing on object detection tasks [70]\n",
      "INFO:root:Title extracted: Table 2 Results of Cutout Regularization augmentation methods, horizontal flipping and cropping\n",
      "INFO:root:Narrative text extracted: [104], plus denotes using traditional\n",
      "INFO:root:Title extracted: Method\n",
      "INFO:root:Title extracted: SVHN\n",
      "INFO:root:Title extracted: ResNetl8 [5]\n",
      "INFO:root:Title extracted: ResNet18\n",
      "INFO:root:Title extracted: cutout\n",
      "INFO:root:Narrative text extracted: WideResNet [21]\n",
      "INFO:root:Title extracted: WideResNet\n",
      "INFO:root:Title extracted: cutout\n",
      "INFO:root:Narrative text extracted: + Shake-shake regularization [4]\n",
      "INFO:root:Title extracted: Shake-shake regularization\n",
      "INFO:root:Title extracted: cutout\n",
      "INFO:root:Narrative text extracted: A 2.56% error rate is obtained on CIFAR-10 using cutout and traditional augmentation methods\n",
      "INFO:root:Narrative text extracted: The italic value denote high performance according to the comparative metrics\n",
      "INFO:root:Narrative text extracted: filling in a missing piece of an image. Using a diverse collection of GAN inpainters, the random erasing augmentation could seed very interesting extrapolations. It will be inter- esting to see if better results can be achieved by erasing different shaped patches such as circles rather than n × m rectangles. An extension of this will be to parameterize the geometries of random erased patches and learn an optimal erasing configuration.\n",
      "INFO:root:Narrative text extracted: A disadvantage to random erasing is that it will not always be a label-preserving trans- formation. In handwritten digit recognition, if the top part of an ‘8’ is randomly cropped out, it is not any different from a ‘6’. In many fine-grained tasks such as the Stanford Cars dataset [73], randomly erasing sections of the image (logo, etc.) may make the car brand unrecognizable. Therefore, some manual intervention may be necessary depending on the dataset and task.\n",
      "INFO:root:Narrative text extracted: A note on combining augmentations\n",
      "INFO:root:Narrative text extracted: Of the augmentations discussed, geometric transformations, color space transforma- tions, kernel filters, mixing images, and random erasing, nearly all of these transforma- tions come with an associated distortion magnitude parameter as well. This parameter encodes the distortional difference between a 45° rotation and a 30° rotation. With a large list of potential augmentations and a mostly continuous space of magnitudes, it is easy to conceptualize the enormous size of the augmentation search space. Combining augmentations such as cropping, flipping, color shifts, and random erasing can result in\n",
      "INFO:root:Narrative text extracted: Fig. 13 Architecture diagram of the feature space augmentation framework presented by DeVries and Taylor [75]\n",
      "INFO:root:Narrative text extracted: Fig. 14 Examples of interpolated instances in the feature space on the handwritten ‘@’ character [75]\n",
      "INFO:root:Narrative text extracted: massively inflated dataset sizes. However, this is not guaranteed to be advantageous. In domains with very limited data, this could result in further overfitting. Therefore, it is important to consider search algorithms for deriving an optimal subset of augmented data to train Deep Learning models with. More on this topic will be discussed in Design Considerations of Data Augmentation.\n",
      "INFO:root:Title extracted: Data Augmentations based on Deep Learning\n",
      "INFO:root:Title extracted: Feature space augmentation\n",
      "INFO:root:Narrative text extracted: All of the augmentation methods discussed above are applied to images in the input space. Neural networks are incredibly powerful at mapping high-dimensional inputs into lower-dimensional representations. These networks can map images to binary classes or to n × 1 vectors in flattened layers. The sequential processing of neural networks can be manipulated such that the intermediate representations can be separated from the network as a whole. The lower-dimensional representations of image data in fully-con- nected layers can be extracted and isolated. Konno and Iwazume [74] find a performance boost on CIFAR-100 from 66 to 73% accuracy by manipulating the modularity of neural networks to isolate and refine individual layers after training. Lower-dimensional repre- sentations found in high-level layers of a CNN are known as the feature space. DeVries and Taylor [75] presented an interesting paper discussing augmentation in this feature space. This opens up opportunities for many vector operations for Data Augmentation.\n",
      "INFO:root:Narrative text extracted: SMOTE is a popular augmentation used to alleviate problems with class imbalance. This technique is applied to the feature space by joining the k nearest neighbors to form new instances. DeVries and Taylor discuss adding noise, interpolating, and extrapolating as common forms of feature space augmentation (Figs. 13, 14).\n",
      "INFO:root:Title extracted: Model\n",
      "INFO:root:Title extracted: MNIST\n",
      "INFO:root:Title extracted: CIFAR-10\n",
      "INFO:root:Title extracted: Baseline\n",
      "INFO:root:Title extracted: Baseline\n",
      "INFO:root:Title extracted: Baseline\n",
      "INFO:root:Title extracted: Baseline\n",
      "INFO:root:Title extracted: input space affine transformations\n",
      "INFO:root:Title extracted: input space extrapolation\n",
      "INFO:root:Title extracted: feature space extrapolation\n",
      "INFO:root:Narrative text extracted: The italic value denote high performance according to the comparative metrics\n",
      "INFO:root:Narrative text extracted: The use of auto-encoders is especially useful for performing feature space augmen- tations on data. Autoencoders work by having one half of the network, the encoder, map images into low-dimensional vector representations such that the other half of the network, the decoder, can reconstruct these vectors back into the original image. This encoded representation is used for feature space augmentation.\n",
      "INFO:root:Narrative text extracted: DeVries and Taylor [75] tested their feature space augmentation technique by extrapo- lating between the 3 nearest neighbors per sample to generate new data and compared their results against extrapolating in the input space and using affine transformations in the input space (Table 3).\n",
      "INFO:root:Narrative text extracted: Feature space augmentations can be implemented with auto-encoders if it is necessary to reconstruct the new instances back into input space. It is also possible to do feature space augmentation solely by isolating vector representations from a CNN. This is done by cutting off the output layer of the network, such that the output is a low-dimensional vector rather than a class label. Vector representations are then found by training a CNN and then passing the training set through the truncated CNN. These vector representa- tions can be used to train any machine learning model from Naive Bayes, Support Vec- tor Machine, or back to a fully-connected multilayer network. The effectiveness of this technique is a subject for future work.\n",
      "INFO:root:Narrative text extracted: A disadvantage of feature space augmentation is that it is very difficult to interpret the vector data. It is possible to recover the new vectors into images using an auto-encoder network; however, this requires copying the entire encoding part of the CNN being trained. For deep CNNs, this results in massive auto-encoders which are very difficult and time-consuming to train. Finally, Wong et al. [76] find that when it is possible to transform images in the data-space, data-space augmentation will outperform feature space augmentation.\n",
      "INFO:root:Title extracted: Adversarial training\n",
      "INFO:root:Narrative text extracted: One of the solutions to search the space of possible augmentations is adversarial training. Adversarial training is a framework for using two or more networks with contrasting objectives encoded in their loss functions. This section will discuss using adversarial training as a search algorithm as well as the phenomenon of adversarial attacking. Adversarial attacking consists of a rival network that learns augmentations to images that result in misclassifications in its rival classification network. These adversarial attacks, constrained to noise injections, have been surprisingly successful from the perspective of the adversarial network. This is surprising because it com- pletely defies intuition about how these models represent images. The adversarial\n",
      "INFO:root:Narrative text extracted: Fig. 15 Adversarial misclassification example [81]\n",
      "INFO:root:Narrative text extracted: attacks demonstrate that representations of images are much less robust than what might have been expected. This is well demonstrated by Moosavi-Dezfooli et al. [77] using DeepFool, a network that finds the minimum possible noise injection needed to cause a misclassification with high confidence. Su et al. [78] show that 70.97% of images can be misclassified by changing just one pixel. Zajac et al. [79] cause mis- classifications with adversarial attacks limited to the border of images. The success of adversarial attacks is especially exaggerated as the resolution of images increases.\n",
      "INFO:root:Narrative text extracted: Adversarial attacking can be targeted or untargeted, referring to the deliberation in which the adversarial network is trying to cause misclassifications. Adversarial attacks can help to illustrate weak decision boundaries better than standard classifica- tion metrics can.\n",
      "INFO:root:Narrative text extracted: In addition to serving as an evaluation metric, defense to adversarial attacks, adver-\n",
      "INFO:root:Narrative text extracted: sarial training can be an effective method for searching for augmentations.\n",
      "INFO:root:Narrative text extracted: By constraining the set of augmentations and distortions available to an adversarial network, it can learn to produce augmentations that result in misclassifications, thus forming an effective search algorithm. These augmentations are valuable for strength- ening weak spots in the classification model. Therefore, adversarial training can be an effective search technique for Data Augmentation. This is in heavy contrast to the traditional augmentation techniques described previously. Adversarial augmentations may not represent examples likely to occur in the test set, but they can improve weak spots in the learned decision boundary.\n",
      "INFO:root:Narrative text extracted: Engstrom et al. [80] showed that simple transformations such as rotations and translations can easily cause misclassifications by deep CNN models. The worst out of the random transformations reduced the accuracy of MNIST by 26%, CIFAR10 by 72% and ImageNet (Top 1) by 28%. Goodfellow et al. [81] generate adversarial exam- ples to improve performance on the MNIST classification task. Using a technique for generating adversarial examples known as the “fast gradient sign method”, a maxout network [82] misclassified 89.4% of adversarial examples with an average confidence of 97.6%. This test is done on the MNIST dataset. With adversarial training, the error rate of adversarial examples fell from 89.4% to 17.9% (Fig. 15).\n",
      "INFO:root:Narrative text extracted: Li et al. [83] experiment with a novel adversarial training approach and compare the performance on original testing data and adversarial examples. The results displayed\n",
      "INFO:root:Narrative text extracted: Table 4 Test accuracies showing the impact of adversarial training, clean refers to the original testing data, FGSM refers to adversary examples derived from Fast Gradient Sign Method and PGD refers to adversarial examples derived from Projected Gradient Descent [83]\n",
      "INFO:root:Title extracted: Models\n",
      "INFO:root:Title extracted: MNIST\n",
      "INFO:root:Title extracted: CIFAR-10\n",
      "INFO:root:Title extracted: Clean\n",
      "INFO:root:Title extracted: FGSM\n",
      "INFO:root:Title extracted: PGD\n",
      "INFO:root:Title extracted: Clean\n",
      "INFO:root:Title extracted: FGSM\n",
      "INFO:root:Title extracted: Standard\n",
      "INFO:root:Title extracted: Adversarially trained\n",
      "INFO:root:Title extracted: Our method\n",
      "INFO:root:Narrative text extracted: below show how anticipation of adversarial attacks in the training process can dramati- cally reduce the success of attacks.\n",
      "INFO:root:Narrative text extracted: As shown in Table 4, the adversarial training in their experiment did not improve the test accuracy. However, it does significantly improve the test accuracy of adversarial examples. Adversarial defense is a very interesting subject for evaluating security and robustness of Deep Learning models. Improving on the Fast Gradient Sign Method, DeepFool, developed by Moosavi-Dezfooli et al. [77], uses a neural network to find the smallest possible noise perturbation that causes misclassifications.\n",
      "INFO:root:Narrative text extracted: Another interesting framework that could be used in an adversarial training context is to have an adversary change the labels of training data. Xie et al. [84] presented Distur- bLabel, a regularization technique that randomly replaces labels at each iteration. This is a rare example of adding noise to the loss layer, whereas most of the other augmenta- tion methods discussed add noise into the input or hidden representation layers. On the MNIST dataset with LeNet [28] CNN architecture, DisturbLabel produced a 0.32% error rate compared to a baseline error rate of 0.39%. DisturbLabel combined with Dropout Regularization produced a 0.28% error rate compared to the 0.39% baseline. To translate this to the context of adversarial training, one network takes in the classifier’s training data as input and learns which labels to flip to maximize the error rate of the classifica- tion network.\n",
      "INFO:root:Narrative text extracted: The effectiveness of adversarial training in the form of noise or augmentation search is still a relatively new concept that has not been widely tested and understood. Adversarial search to add noise has been shown to improve performance on adversarial examples, but it is unclear if this is useful for the objective of reducing overfitting. Future work seeks to expand on the relationship between resistance to adversarial attacks and actual performance on test datasets.\n",
      "INFO:root:Title extracted: GAN‑based Data Augmentation\n",
      "INFO:root:Narrative text extracted: Another exciting strategy for Data Augmentation is generative modeling. Genera- tive modeling refers to the practice of creating artificial instances from a dataset such that they retain similar characteristics to the original set. The principles of adversarial training discussed above have led to the very interesting and massively popular genera- tive modeling framework known as GANs. Bowles et al. [85] describe GANs as a way to “unlock” additional information from a dataset. GANs are not the only generative\n",
      "INFO:root:Title extracted: PGD\n",
      "INFO:root:Narrative text extracted: Fig. 16 Illustration of GAN concept provided by Mikolajczyk and Grochowski [72]\n",
      "INFO:root:Narrative text extracted: modeling technique that exists; however they are dramatically leading the way in com- putation speed and quality of results.\n",
      "INFO:root:Narrative text extracted: Another useful strategy for generative modeling worth mentioning is variational auto-encoders. The GAN framework can be extended to improve the quality of samples produced with variational auto-encoders [86]. Variational auto-encoders learn a low- dimensional representation of data points. In the image domain, this translates an image tensor of size height × width × color channels down into a vector of size n × 1, identi- cal to what was discussed with respect to feature space augmentation. Low-dimensional constraints in vector representations will result in a poorer representation, although these constraints are better for visualization using methods such as t-SNE [87]. Imag- ine a vector representation of size 5 × 1 created by an autoencoder. These autoencoders can take in a distribution of labeled data and map them into this space. These classes could include ‘head turned left’, ‘centered head’, and ‘head turned right’. The auto-encoder learns a low-dimensional representation of these data points such that vector operations such as adding and subtracting can be used to simulate a front view-3D rotation of a new instance. Variational auto-encoder outputs can be further improved by inputting them into GANs [31]. Additionally, a similar vector manipulation process can be done on the noise vector inputs to GANs through the use of Bidirectional GANs [88].\n",
      "INFO:root:Narrative text extracted: The impressive performance of GANs has resulted in increased attention on how they can be applied to the task of Data Augmentation. These networks have the ability to gen- erate new training data that results in better performing classification models. The GAN architecture first proposed by Ian Goodfellow [31] is a framework for generative mode- ling through adversarial training. The best anecdote for understanding GANs is the anal- ogy of a cop and a counterfeiter. The counterfeiter (generator network) takes in some form of input. This could be a random vector, another image, text, and many more. The counterfeiter learns to produce money such that the cop (discriminator network) cannot tell if the money is real or fake. The real or fake dichotomy is analogous to whether or not the generated instance is from the training set or if it was created by the generator network (Fig. 16).\n",
      "INFO:root:Narrative text extracted: The counterfeiter versus robber analogy is a seamless bridge to understand GANs in the context of network intrusion detection. Lin et al. [89] use a generator network to learn how to fool a black-box detection system. This highlights one of the most interesting characteristics of GANs. Analysis tools derived from game theory such as minimax strategy and the Nash Equilibrium [90] suggest that the generator will even- tually fool the discriminator. The success of the generator to overcome the discrimi- nator makes it very powerful for generative modeling. GANs are the most promising generative modeling technique for use in Data Augmentation.\n",
      "INFO:root:Narrative text extracted: The vanilla GAN architecture uses multilayer perceptron networks in the gen- erator and discriminator networks. This is able to produce acceptable images on a simple image dataset such as the MNIST handwritten digits. However, it fails to produce quality results for higher resolution, more complicated datasets. In the MNIST dataset, each image is only 28 × 28 × 1 for a total of 784 pixels. GANs applied to the MNIST data are able to produce convincing results. However, MNIST images are far less challenging than other image datasets due to low intra-class vari- ance and resolution, to name a couple differences of many. This is in heavy contrast with other datasets studied in most academic Computer Vision papers such as Ima- geNet or CIFAR-10. For immediate reference, an ImageNet image is of resolution 256 × 256 × 3, totaling 196,608 pixels, a 250× increase in pixel count compared with MNIST.\n",
      "INFO:root:Narrative text extracted: Many research papers have been published that modify the GAN framework through different network architectures, loss functions, evolutionary methods, and many more. This research has significantly improved the quality of samples created by GANs. There have been many new architectures proposed for expanding on the con- cept of GANs and producing higher resolution output images, many of which are out of the scope of this paper. Amongst these new architectures, DCGANs, Progressively Growing GANs, CycleGANs, and Conditional GANs seem to have the most applica- tion potential in Data Augmentation.\n",
      "INFO:root:Narrative text extracted: The DCGAN [91] architecture was proposed to expand on the internal complex- ity of the generator and discriminator networks. This architecture uses CNNs for the generator and discriminator networks rather than multilayer perceptrons. The DCGAN was tested to generate results on the LSUN interior bedroom image data- set, each image being 64 × 64 × 3, for a total of 12,288 pixels, (compared to 784 in MNIST). The idea behind DCGAN is to increase the complexity of the generator network to project the input into a high dimensional tensor and then add decon- volutional layers to go from the projected tensor to an output image. These decon- volutional layers will expand on the spatial dimensions, for example, going from 14 × 14 × 6 to 28 × 28 × 1, whereas a convolutional layer will decrease the spatial dimensions such as going from 14 × 14 × 32 to 7 × 7 × 64. The DCGAN architecture presents a strategy for using convolutional layers in the GAN framework to produce higher resolution images (Figs. 17, 18).\n",
      "INFO:root:Narrative text extracted: Fig. 17 DCGAN, generator architecture presented by Radford et al. [91]\n",
      "INFO:root:Narrative text extracted: Fig. 18 Complete DCGAN architecture used by Frid-Adar et al. [49] to generate liver lesion images\n",
      "INFO:root:Narrative text extracted: Frid-Adar et al. [49] tested the effectiveness of using DCGANs to generate liver lesion medical images. They use the architecture pictured above to generate 64 × 64 × 1 size images of liver lesion CT scans. Their original dataset contains 182 CT scans, (53 Cysts, 64 Metastases, and 65 Hemangiomas). After using classical augmentations to achieve 78.6% sensitivity and 88.4% specificity, they observed an increase to 85.7% sen- sitivity and 92.4% specificity once they added the DCGAN-generated samples.\n",
      "INFO:root:Narrative text extracted: Another architecture of interest is known as Progressively Growing GANs [34]. This architecture trains a series of networks with progressive resolution complexity. These resolutions range from 4 × 4 to 8 × 8 and so on until outputs of size 1024 × 1024 are achieved. This is built on the concept that GANs can accept images as input as well as random vectors. Therefore, the series of GANs work by passing samples from a lower resolution GAN up to higher-resolution GANs. This has produced very amaz- ing results on facial images.\n",
      "INFO:root:Narrative text extracted: In addition to improving the resolution size of GANs, another interesting architecture that increases the quality of outputs is the CycleGAN [92] proposed by Zhu et al. Cycle- GAN introduces an additional Cycle-Consistency loss function to help stabilize GAN training. This is applied to image-to-image translation. Neural Style Transfer [32], dis- cussed further in the section below, learns a single image to single image translation. However, CycleGAN learns to translate from a domain of images to another domain, such as horses to zebras. This is implemented via forward and backward consistency loss functions. A generator takes in images of horses and learns to map them to zebras such that the discriminator cannot tell if they were originally a part of the zebra set or not, as discussed above. After this, the generated zebras from horse images are passed through a network which translates them back into horses. A second discriminator determines if this re-translated image belongs to the horse set or not. Both of these discriminator losses are aggregated to form the cycle-consistency loss.\n",
      "INFO:root:Narrative text extracted: The use of CycleGANs was tested by Zhu et al. [93] in the task of Emotion Clas- sification. Using the emotion recognition dataset, FER2013 [94], Facial Expression Recognition Database, they build a CNN classifier to recognize 7 different emotions: angry, disgust, fear, happy, sad, surprise, and neutral. These classes are imbalanced and the CycleGAN is used as a method of intelligent oversampling.\n",
      "INFO:root:Narrative text extracted: CycleGANs learned an unpaired image-to-image translation between domains. An example of the domains in this problem is neutral to disgust. The CycleGAN learns to translate an image representing a neutral image into an image representing the dis- gust emotion (Figs. 19, 20).\n",
      "INFO:root:Narrative text extracted: Fig. 19 Some examples of synthetic data created with CycleGANs for emotion classification\n",
      "INFO:root:Narrative text extracted: Fig. 20 Architecture overview: G and F consist of two separate GANs composing the CycleGAN. Two images are taken from the reference and target class and used to generate new data in the target class\n",
      "INFO:root:Narrative text extracted: Using CycleGANs to translate images from the other 7 classes into the minority classes was very effective in improving the performance of the CNN model on emo- tion recognition. Employing these techniquess, accuracy improved 5–10%. To further understand the effectiveness of adding GAN-generated instances, a t-SNE visualiza- tion is used. t-SNE [87] is a visualization technique that learns to map between high- dimensional vectors into a low-dimensional space to facilitate the visualization of decision boundaries (Fig. 21).\n",
      "INFO:root:Narrative text extracted: Another interesting GAN architecture for use in Data Augmentation is Conditional GANs [95]. Conditional GANs add a conditional vector to both the generator and the discriminator in order to alleviate problems with mode collapse. In addition to inputting a random vector z to the generator, Conditional GANs also input a y vector which could be something like a one-hot encoded class label, e.g. [0 0 0 1 0]. This class label targets a specific class for the generator and the discriminator (Fig. 22).\n",
      "INFO:root:Narrative text extracted: Fig. 21 t-SNE visualization demonstrating the improved decision boundaries when using CycleGAN-generated samples. a original CNN model, b adding GAN-generated disgust images, c adding GAN-generated sad images, d adding both GAN-generated disgust and sad images [93]\n",
      "INFO:root:Narrative text extracted: Fig. 22 Illustration from Mirza and Osindero [95] showing how the conditional y vector is integrated into the GAN framework\n",
      "INFO:root:Narrative text extracted: Lucic et al. [96] sought out to compare newly developed GAN loss functions. They conducted a series of tests that determined most loss functions can reach similar scores with enough hyperparameter optimization and random restarts. This suggests that increased computational power is a more promising area of focus than algorithmic changes in the generator versus discriminator loss function.\n",
      "INFO:root:Narrative text extracted: Most of the research done in applying GANs to Data Augmentation and reporting the resulting classification performance has been done in biomedical image analysis\n",
      "INFO:root:Narrative text extracted: [39]. These papers have shown improved classification boundaries derived from train- ing with real and generated data from GAN models. In addition, some papers measure the quality of GAN outputs by a visual Turing test. In these tests, the study asks two experts to distinguish between real and artificial images in medical image tasks such as skin lesion classification and liver cancer detection. Table 5 shows that the first and second experts were only able to correctly label 62.5% and 58.6% of the GAN-gener- ated liver lesion images as fake. Labeling images as fake refers to their origin coming from the generator rather than an actual liver lesion image (Table 6; Fig. 23).\n",
      "INFO:root:Narrative text extracted: Table 5 Results of ‘Visual Turing Test’ on DCGAN-generated liver lesion images presented by Frid-Adar et al. [139]\n",
      "INFO:root:Title extracted: Classification accuracy\n",
      "INFO:root:Title extracted: Is ROI real?\n",
      "INFO:root:Title extracted: Real (%)\n",
      "INFO:root:Title extracted: Synthetic (%)\n",
      "INFO:root:Title extracted: Total score\n",
      "INFO:root:Title extracted: Total score\n",
      "INFO:root:Title extracted: Expert 1\n",
      "INFO:root:Title extracted: Expert 2\n",
      "INFO:root:Narrative text extracted: Table 6 Results of ‘Visual Turing Test’ on different DCGAN- and WGAN [104]—generated brain tumor MR images presented by Han et al. [140]\n",
      "INFO:root:Title extracted: Accuracy (%)\n",
      "INFO:root:Narrative text extracted: Real selected as real\n",
      "INFO:root:Title extracted: Real as synt\n",
      "INFO:root:Title extracted: Synt as real\n",
      "INFO:root:Title extracted: Synt as synt\n",
      "INFO:root:Title extracted: T1 (DCGAN, 128\n",
      "INFO:root:Title extracted: Tlc (DCGAN, 128\n",
      "INFO:root:Title extracted: T2 (DCGAN, 128\n",
      "INFO:root:Title extracted: FLAIR (DCGAN, 128\n",
      "INFO:root:Title extracted: × Concat (DCGAN, 128\n",
      "INFO:root:Title extracted: Concat (DCGAN, 64\n",
      "INFO:root:Title extracted: Tlc (WGAN, 128\n",
      "INFO:root:Title extracted: FLAIR (WGAN, 128\n",
      "INFO:root:Title extracted: × Concat (WGAN, 128\n",
      "INFO:root:Title extracted: Concat (WGAN, 64\n",
      "INFO:root:Narrative text extracted: Fig. 23 Trends in applying GANs to medical image analysis [39]\n",
      "INFO:root:Narrative text extracted: Fig. 24 Adversarial autoencoder framework used in DOPING [97]\n",
      "INFO:root:Narrative text extracted: GAN samples can be used as an oversampling technique to solve problems with class imbalance. Lim et al. [97] show how GAN samples can be used for unsupervised anom- aly detection. By oversampling rare normal samples, which are samples that occur with small probability, GANs are able to reduce the false positive rate of anomaly detection. They do this using the Adversarial Autoencoder framework proposed by Makhzani et al. [98] (Fig. 24).\n",
      "INFO:root:Narrative text extracted: As exciting as the potential of GANs is, it is very difficult to get high-resolution out- puts from the current cutting-edge architectures. Increasing the output size of the images produced by the generator will likely cause training instability and non-conver- gence. Another drawback of GANs is that they require a substantial amount of data to train. Thus, depending on how limited the initial dataset is, GANs may not be a practical solution. Salimans et al. [99] provide a more complete description of the problems with training GANs.\n",
      "INFO:root:Title extracted: Neural Style Transfer\n",
      "INFO:root:Narrative text extracted: Neural Style Transfer [32] is one of the flashiest demonstrations of Deep Learning capabilities. The general idea is to manipulate the representations of images created in CNNs. Neural Style Transfer is probably best known for its artistic applications, but it also serves as a great tool for Data Augmentation. The algorithm works by manipulat- ing the sequential representations across a CNN such that the style of one image can be transferred to another while preserving its original content. A more detailed explanation of the gram matrix operation powering Neural Style Transfer can be found by Li et al. [100] (Fig. 25).\n",
      "INFO:root:Narrative text extracted: It is important to also recognize an advancement of the original algorithm from Gatys et al. known as Fast Style Transfer [35]. This algorithm extends the loss function from a per-pixel loss to a perceptual loss and uses a feed-forward network to stylize images. This perceptual loss is reasoned about through the use of another pre-trained net. The use of perceptual loss over per-pixel loss has also shown great promise in the applica- tion of super-resolution [101] as well as style transfer. This loss function enhancement enables style transfer to run much faster, increasing interest in practical applications. Additionally, Ulyanov et al. [102] find that replacing batch normalization with instance normalization results in a significant improvement for fast stylization (Fig. 26).\n",
      "INFO:root:Narrative text extracted: For the purpose of Data Augmentation, this is somewhat analogous to color space lighting transformations. Neural Style Transfer extends lighting variations and enables the encoding of different texture and artistic styles as well. This leaves practitioners of Data Augmentation with the decision of which styles to sample from when deriving new images via Neural Style Transfer.\n",
      "INFO:root:Narrative text extracted: Choosing which styles to sample from can be a challenging task. For applica- tions such as self-driving cars it is fairly intuitive to think of transferring training\n",
      "INFO:root:Narrative text extracted: Fig. 25 Illustration of style and content reconstructions in Neural Style Transfer [32]\n",
      "INFO:root:Narrative text extracted: data into a night-to-day scale, winter-to-summer, or rainy-to-sunny scale. However, in other application domains, the set of styles to transfer into is not so obvious. For ease of implementation, data augmentation via Neural Style Transfer could be done by selecting a set of k styles and applying them to all images in the training set. The work of Style Augmentation [103], avoids introducing a new form of style bias into the dataset by deriving styles at random from a distribution of 79,433 artistic images. Transferring style in training data has been tested on the transition from simulated environments to the real-world. This is very useful for robotic manipulation tasks using Reinforcement Learning because of potential damages to hardware when train- ing in the real-world. Many constraints such as low-fidelity cameras cause these models to generalize poorly when trained in physics simulations and deployed in the real-world.\n",
      "INFO:root:Narrative text extracted: Tobin et al. [104] explore the effectiveness of using different styles in training simula- tion and achieve within 1.5 cm accuracy in the real-world on the task of object localiza- tion. Their experiments randomize the position and texture of the objects to be detected\n",
      "INFO:root:Narrative text extracted: Fig. 27 Examples of different styles simulated by Tobin et al. [104]\n",
      "INFO:root:Narrative text extracted: on the table in the simulation, as well as the texture, lighting, number of lights, and ran- dom noise in the background. They found that with enough variability in the training data style, the real-world simply appears as another variation to the model. Interestingly, they found that diversity in styles was more effective than simulating in as realistic of an environment as possible. This is in contrast to the work of Shrivastava et al. [105] who used GANs to make their simulated data as realistic as possible (Fig. 27).\n",
      "INFO:root:Narrative text extracted: Using simulated data to build Computer Vision models has been heavily investigated. One example of this is from Richter et al. [106]. They use computer graphics from mod- ern open-world games such as Grand Theft Auto to produce semantic segmentation datasets. The authors highlight anecdotes of the manual annotation costs required to build these pixel-level datasets. They mention the CamVid dataset [107] requires 60 min per image to manually annotate, and the Cityscapes dataset [108] requires 90 min per image. This high labor and time cost motivates the use and development of synthetic datasets. Neural Style Transfer is a very interesting strategy to improve the generaliza- tion ability of simulated datasets.\n",
      "INFO:root:Narrative text extracted: A disadvantage of Neural Style Transfer Data Augmentation is the effort required to select styles to transfer images into. If the style set is too small, further biases could be introduced into the dataset. Trying to replicate the experiments of Tobin et al. [104] will require a massive amount of additional memory and compute to transform and store 79,433 new images from each image. The original algorithm proposed by Gatys et al. [32] has a very slow running time and is therefore not practical for Data Augmentation. The algorithm developed by Johnson et al. [35] is much faster, but limits transfer to a pre-trained set of styles.\n",
      "INFO:root:Title extracted: Meta learning Data Augmentations\n",
      "INFO:root:Narrative text extracted: The concept of meta-learning in Deep Learning research generally refers to the concept of optimizing neural networks with neural networks. This approach has become very popular since the publication of NAS [33] from Zoph and Le. Real et al. [109, 110] also\n",
      "INFO:root:Title extracted: Fig. 28 Concept behind Neural Architecture Search [33]\n",
      "INFO:root:Narrative text extracted: show the effectiveness of evolutionary algorithms for architecture search. Salimans et al. [111] directly compare evolutionary strategies with Reinforcement Learning. Another interesting alternative to Reinforcement Learning is simple random search [112]. Uti- lizing evolutionary and random search algorithms is an interesting area of future work, but the meta-learning schemes reviewed in this survey are all neural-network, gradient-based.\n",
      "INFO:root:Narrative text extracted: The history of Deep Learning advancement from feature engineering such as SIFT [113] and HOG [114] to architecture design such as AlexNet [1], VGGNet [2], and Inception-V3 [4], suggest that meta-architecture design is the next paradigm shift. NAS takes a novel approach to meta-learning architectures by using a recurrent net- work trained with Reinforcement Learning to design architectures that result in the best accuracy. On the CIFAR-10 dataset, this achieved an error rate of 3.65 (Fig. 28).\n",
      "INFO:root:Narrative text extracted: This section will introduce three experiments using meta-learning for Data Aug- mentation. These methods use a prepended neural network to learn Data Augmenta- tions via mixing images, Neural Style Transfer, and geometric transformations.\n",
      "INFO:root:Narrative text extracted: Neural augmentation The Neural Style Transfer algorithm requires two parameters for the weights of the style and content loss. Perez and Wang [36] presented an algo- rithm to meta-learn a Neural Style Transfer strategy called Neural Augmentation. The Neural Augmentation approach takes in two random images from the same class. The prepended augmentation net maps them into a new image through a CNN with 5 layers, each with 16 channels, 3 × 3 filters, and ReLU activation functions. The image outputted from the augmentation is then transformed with another random image via Neural Style Transfer. This style transfer is carried out via the CycleGAN [92] exten- sion of the GAN [31] framework. These images are then fed into a classification model and the error from the classification model is backpropagated to update the Neural Augmentation net. The Neural Augmentation network uses this error to learn the opti- mal weighting for content and style images between different images as well as the mapping between images in the CNN (Fig. 29).\n",
      "INFO:root:Narrative text extracted: Perez and Wang tested their algorithm on the MNIST and Tiny-imagenet-200 data- sets on binary classification tasks such as cat versus dog. The Tiny-imagenet-200 dataset is used to simulate limited data. The Tiny-imagenet-200 dataset contains only 500 images in each of the classes, with 100 set aside for validation. This problem\n",
      "INFO:root:Narrative text extracted: Fig. 29 Illustration of augmentation network [36]\n",
      "INFO:root:Narrative text extracted: limits this dataset to 2 classes. Thus there are only 800 images for training. Each of the Tiny-imagenet-200 images is 64 × 64 × 3, and the MNIST images are 28 × 28 × 1. The experiment compares their proposed Neural Augmentation [36] approach with traditional augmentation techniques such as cropping and rotation, as well as with a style transfer approach with a predetermined set of styles such as Night/Day and Winter/Summer.\n",
      "INFO:root:Narrative text extracted: The traditional baseline study transformed images by choosing an augmentation from a set (shifted, zoomed in/out, rotated, flipped, distorted, or shaded with a hue). This was repeated to increase the dataset size from N to 2 N. The GAN style transfer baseline uses 6 different styles to transform images (Cezanne, Enhance, Monet, Uki- yoe, Van Gogh and Winter). The Neural Augmentation techniques tested consist of three levels based on the design of the loss function for the augmentation net (Con- tent loss, Style loss via gram matrix, and no loss computer at this layer). All experi- ments are tested with a convolutional network consisting of 3 convolutional layers each followed by max pooling and batch normalization, followed by 2 fully-connected layers. Each experiment runs for 40 epochs at a learning rate of 0.0001 with the Adam optimization technique (Table 7).\n",
      "INFO:root:Narrative text extracted: The results of the experiment are very promising. The Neural Augmentation tech- nique performs significantly better on the Dogs versus Goldfish study and only slightly worse on Dogs versus Cats. The technique does not have any impact on the MNIST problem. The paper suggests that the likely best strategy would be to combine the traditional augmentations and the Neural Augmentations.\n",
      "INFO:root:Narrative text extracted: Smart Augmentation The Smart Augmentation [37] approach utilizes a similar con- cept as the Neural Augmentation technique presented above. However, the combina- tion of images is derived exclusively from the learned parameters of a prepended CNN, rather than using the Neural Style Transfer algorithm.\n",
      "INFO:root:Narrative text extracted: Smart Augmentation is another approach to meta-learning augmentations. This is done by having two networks, Network-A and Network-B. Network-A is an augmen- tation network that takes in two or more input images and maps them into a new image or images to train Network-B. The change in the error rate in Network-B is then\n",
      "INFO:root:Narrative text extracted: Table 7 Results comparing augmentations [36]\n",
      "INFO:root:Title extracted: Quantitative results on dogs vs. goldfish\n",
      "INFO:root:Narrative text extracted: Dogs vs goldfish\n",
      "INFO:root:Title extracted: Augmentation\n",
      "INFO:root:Title extracted: None\n",
      "INFO:root:Title extracted: Traditional\n",
      "INFO:root:Title extracted: GANs\n",
      "INFO:root:Title extracted: Neural\n",
      "INFO:root:Title extracted: Neural\n",
      "INFO:root:Title extracted: Neural\n",
      "INFO:root:Title extracted: Control\n",
      "INFO:root:Title extracted: no loss\n",
      "INFO:root:Title extracted: content loss\n",
      "INFO:root:Title extracted: style\n",
      "INFO:root:Title extracted: Quantitative results on dogs vs cats\n",
      "INFO:root:Narrative text extracted: Dogs vs cat\n",
      "INFO:root:Title extracted: Augmentation\n",
      "INFO:root:Title extracted: None\n",
      "INFO:root:Title extracted: Traditional\n",
      "INFO:root:Title extracted: GANs\n",
      "INFO:root:Title extracted: Neural\n",
      "INFO:root:Title extracted: Neural\n",
      "INFO:root:Title extracted: Neural\n",
      "INFO:root:Title extracted: Control\n",
      "INFO:root:Title extracted: no loss\n",
      "INFO:root:Title extracted: content loss\n",
      "INFO:root:Title extracted: style\n",
      "INFO:root:Title extracted: MNIST 0′s and 8′s\n",
      "INFO:root:Title extracted: Augmentation\n",
      "INFO:root:Title extracted: None\n",
      "INFO:root:Title extracted: Neural\n",
      "INFO:root:Title extracted: Neural\n",
      "INFO:root:Title extracted: no loss\n",
      "INFO:root:Title extracted: content loss\n",
      "INFO:root:Narrative text extracted: backpropagated to update Network-A. Additionally another loss function is incorpo- rated into Network-A to ensure that its outputs are similar to others within the class. Network-A uses a series of convolutional layers to produce the augmented image. The conceptual framework of Network-A can be expanded to use several Networks trained in parallel. Multiple Network-As could be very useful for learning class-specific aug- mentations via meta-learning (Fig. 30).\n",
      "INFO:root:Narrative text extracted: Smart Augmentation is similar to SamplePairing [65] or mixed-examples in the sense that a combination of existing examples produces new ones. However, the mechanism of Smart Augmentation is much more sophisticated, using an adaptive CNN to derive new images rather than averaging pixels or hand-engineered image combinations.\n",
      "INFO:root:Narrative text extracted: The Smart Augmentation technique was tested on the task of gender recognition. On the Feret dataset, accuracy improved from 83.52 to 88.46%. The audience dataset responded with an improvement of 70.02% to 76.06%. Most interestingly, results from another face dataset increased from 88.15 to 95.66%. This was compared with traditional augmentation techniques which increased the accuracy from 88.15 to 89.08%. Addition- ally, this experiment derived the same accuracy when using two Network-As in the aug- mentation framework as was found with one Network-A. This experiment demonstrates\n",
      "INFO:root:Title extracted: Val. acc.\n",
      "INFO:root:Title extracted: Val. acc.\n",
      "INFO:root:Title extracted: Val. acc.\n",
      "INFO:root:Narrative text extracted: ] 7 3 [ e r u t c e t i h c r a n o i t a t n e m g u A t r a m S e h t\n",
      "INFO:root:Title extracted: f\n",
      "INFO:root:Narrative text extracted: o n o i t a r t s u\n",
      "INFO:root:Title extracted: l l I\n",
      "INFO:root:Title extracted: g i F\n",
      "INFO:root:Narrative text extracted: Fig. 31 On the gender recognition task, the image to the left is an example of an instance produced by Network-A in Smart Augmentation given the right images as input [37]\n",
      "INFO:root:Narrative text extracted: AutoAugment AutoAugment [38], developed by Cubuk et al., is a much different approach to meta-learning than Neural Augmentation or Smart Augmentation. Auto- Augment is a Reinforcement Learning algorithm [115] that searches for an optimal augmentation policy amongst a constrained set of geometric transformations with mis- cellaneous levels of distortions. For example, ‘translateX 20 pixels’ could be one of the transformations in the search space (Table 8).\n",
      "INFO:root:Narrative text extracted: In Reinforcement Learning algorithms, a policy is analogous to the strategy of the learning algorithm. This policy determines what actions to take at given states to achieve some goal. The AutoAugment approach learns a policy which consists of many sub- policies, each sub-policy consisting of an image transformation and a magnitude of transformation. Reinforcement Learning is thus used as a discrete search algorithm of augmentations. The authors also suggest that evolutionary algorithms or random search would be effective search algorithms as well.\n",
      "INFO:root:Narrative text extracted: AutoAugment found policies which achieved a 1.48% error rate on CIFAR-10. Auto- Augment also achieved an 83.54% Top-1 accuracy on the ImageNet dataset. Very inter- estingly as well, the policies learned on the ImageNet dataset were successful when transferred to the Stanford Cars and FGVC Aircraft image recognition tasks. In this case, the ImageNet policy applied to these other datasets reduced error rates by 1.16% and 1.76% respectively.\n",
      "INFO:root:Narrative text extracted: Geng et al. [116] expanded on AutoAugment by replacing the Reinforcement Learning search algorithm with Augmented Random Search (ARS) [112]. The authors point out that the sub-policies learned from AutoAugment are inherently flawed because of the discrete search space. They convert the probability and magnitude of augmentations into a continuous space and search for sub-policies with ARS. With this, they achieve lower error rates on CIFAR-10, CIFAR-100, and ImageNet (Table 9).\n",
      "INFO:root:Narrative text extracted: Minh et al. [117] also experimented with using Reinforcement Learning [115] to search for Data Augmentations. They further explore the effectiveness of learning trans- formations for individual instances rather than the entire dataset. They find classifica- tion accuracy differences of 70.18% versus 74.42% on the CIFAR-10 dataset and 74.61% versus 80.35% on the problem of classifying dogs versus cats. Further, they explore the robustness of classifiers with respect to test-time augmentation and find that the model\n",
      "INFO:root:Narrative text extracted: Table 8 AutoAugment augmentation policy found on the reduced CIFAR-10 dataset [38]\n",
      "INFO:root:Title extracted: Operation 1\n",
      "INFO:root:Title extracted: Operation 2\n",
      "INFO:root:Title extracted: Sub-policy 0\n",
      "INFO:root:Title extracted: (Contrast,0.2,6)\n",
      "INFO:root:Title extracted: Sub-policy 1\n",
      "INFO:root:Title extracted: (TranslateX,0.3,9)\n",
      "INFO:root:Title extracted: Sub-policy 2\n",
      "INFO:root:Title extracted: (Sharpness,0.8,1)\n",
      "INFO:root:Title extracted: (Sharpness,0.9,3)\n",
      "INFO:root:Title extracted: Sub-policy 3\n",
      "INFO:root:Title extracted: (TranslateY,0.7,9)\n",
      "INFO:root:Title extracted: Sub-policy 4\n",
      "INFO:root:Title extracted: (AutoContrast,0.5,8)\n",
      "INFO:root:Title extracted: (Equalize,0.9,2)\n",
      "INFO:root:Title extracted: Sub-policy 5\n",
      "INFO:root:Title extracted: (Posterize,0.3,7)\n",
      "INFO:root:Title extracted: Sub-policy 6\n",
      "INFO:root:Title extracted: (Brightness,0.6,7)\n",
      "INFO:root:Title extracted: Sub-policy 7\n",
      "INFO:root:Title extracted: (Sharpness,0.3,9)\n",
      "INFO:root:Title extracted: (Brightness,0.7,9)\n",
      "INFO:root:Title extracted: Sub-policy 8\n",
      "INFO:root:Title extracted: (Equalize,0.6,5)\n",
      "INFO:root:Title extracted: (Equalize,0.5,1)\n",
      "INFO:root:Title extracted: Sub-policy 9\n",
      "INFO:root:Title extracted: (Contrast,0.6,7)\n",
      "INFO:root:Title extracted: (Sharpness,0.6,5)\n",
      "INFO:root:Title extracted: Sub-policy 10\n",
      "INFO:root:Title extracted: (TranslateX,0.5,8)\n",
      "INFO:root:Title extracted: Sub-policy 11\n",
      "INFO:root:Title extracted: (Equalize,0.3,7)\n",
      "INFO:root:Title extracted: (AutoContrast,0.4,8)\n",
      "INFO:root:Title extracted: Sub-policy 12\n",
      "INFO:root:Title extracted: (TranslateY,0.4,3)\n",
      "INFO:root:Title extracted: (Sharpness,0.2,6)\n",
      "INFO:root:Title extracted: Sub-policy 13\n",
      "INFO:root:Title extracted: (Brightness,0.9,6)\n",
      "INFO:root:Title extracted: Sub-policy 14\n",
      "INFO:root:Title extracted: (Solarize,0.5,2)\n",
      "INFO:root:Title extracted: Sub-policy 15\n",
      "INFO:root:Title extracted: (Equalize,0.2,0)\n",
      "INFO:root:Title extracted: (AutoContrast,0.6,0)\n",
      "INFO:root:Title extracted: Sub-policy 16\n",
      "INFO:root:Title extracted: (Equalize,0.2,8)\n",
      "INFO:root:Title extracted: (Equalize,0.6,4)\n",
      "INFO:root:Title extracted: Sub-policy 17\n",
      "INFO:root:Title extracted: (Equalize,0.6,6)\n",
      "INFO:root:Title extracted: Sub-policy 18\n",
      "INFO:root:Title extracted: (AutoContrast,0.8,4)\n",
      "INFO:root:Title extracted: (Solarize,0.2,8)\n",
      "INFO:root:Title extracted: Sub-policy 19\n",
      "INFO:root:Title extracted: (Brightness,0.1,3)\n",
      "INFO:root:Title extracted: Sub-policy 20\n",
      "INFO:root:Title extracted: (Solarize,0.4,5)\n",
      "INFO:root:Title extracted: (AutoContrast,0.9,3)\n",
      "INFO:root:Title extracted: Sub-policy 21\n",
      "INFO:root:Title extracted: (TranslateY,0.9,9)\n",
      "INFO:root:Title extracted: (TranslateY,0.7,9)\n",
      "INFO:root:Title extracted: Sub-policy 22\n",
      "INFO:root:Title extracted: (AutoContrast,0.9,2)\n",
      "INFO:root:Title extracted: (Solarize,0.8,3)\n",
      "INFO:root:Title extracted: Sub-policy 23\n",
      "INFO:root:Title extracted: (Equalize,0.8,8)\n",
      "INFO:root:Title extracted: Sub-policy 24\n",
      "INFO:root:Title extracted: (TranslateY,0.7,9)\n",
      "INFO:root:Title extracted: (AutoContrast,0.9,1)\n",
      "INFO:root:Title extracted: Model\n",
      "INFO:root:Title extracted: AutoAugment\n",
      "INFO:root:Title extracted: ARS-Aug\n",
      "INFO:root:Title extracted: Wide-ResNet-28-10\n",
      "INFO:root:Title extracted: Shake-Shake (26 2\n",
      "INFO:root:Title extracted: Shake-Shake (26 2\n",
      "INFO:root:Title extracted: Shake-Shake (26 2\n",
      "INFO:root:Title extracted: × AmoebaNet-B (6,128)\n",
      "INFO:root:Title extracted: 32 days)\n",
      "INFO:root:Title extracted: 96 days)\n",
      "INFO:root:Title extracted: 112 days)\n",
      "INFO:root:Title extracted: PyramidNet\n",
      "INFO:root:Title extracted: ShakeDrop\n",
      "INFO:root:Narrative text extracted: trained with Reinforcement Learning augmentation search performs much better. On the CIFAR-10 dataset this results in 50.99% versus 70.06% accuracy when the models are evaluated on augmented test data.\n",
      "INFO:root:Narrative text extracted: A disadvantage to meta-learning is that it is a relatively new concept and has not been heavily tested. Additionally, meta-learning schemes can be difficult and time-consuming to implement. Practitioners of meta-learning will have to solve problems primarily with vanishing gradients [118], amongst others, to train these networks.\n",
      "INFO:root:Title extracted: Comparing Augmentations\n",
      "INFO:root:Narrative text extracted: As shown throughout “Design considerations for image Data Augmentation” section, possibilities for Data Augmentation. However, there are not many comparative studies that show the performance differences of these different augmentations. One such study was conducted by Shijie et al. [119] which compared GANs, WGANs, flipping, cropping, shifting, PCA jittering, color jittering, adding noise, rotation, and some combinations on the CIFAR-10 and ImageNet datasets. Additionally, the comparative study ranged across dataset sizes with the small set consisting of 2 k samples with 200 in each class, tthe medium set consisting of 10 k samples with 1 k in each class, and the large set consist- ing of 50 k samples with 5 k in each class. They also tested with 3 levels of augmentation, no augmentation, original plus same size of generated samples, and original plus double size of generated samples. They found that cropping, flipping, WGAN, and rotation gen- erally performed better than others. The combinations of flipping + cropping and flip- ping + WGAN were the best overall, improving classification performance on CIFAR-10 by + 3% and + 3.5%, respectively.\n",
      "INFO:root:Title extracted: Design considerations for image Data Augmentation\n",
      "INFO:root:Narrative text extracted: This section will briefly describe some additional design decisions with respect to Data Augmentation techniques on image data.\n",
      "INFO:root:Title extracted: Test-time augmentation\n",
      "INFO:root:Narrative text extracted: In addition to augmenting training data, many research reports have shown the effec- tiveness of augmenting data at test-time as well. This can be seen as analogous to ensem- ble learning techniques in the data space. By taking a test image and augmenting it in the same way as the training images, a more robust prediction can be derived. This comes at a computational cost depending on the augmentations performed, and it can restrict the speed of the model. This could be a very costly bottleneck in models that require real-time prediction. However, test-time augmentation is a promising practice for appli- cations such as medical image diagnosis. Radosavovic et al. [120] denote test-time aug- mentation as data distillation to describe the use of ensembled predictions to get a better representation of the image.\n",
      "INFO:root:Narrative text extracted: Wang et al. [121] sought out to develop a mathematical framework to formulate test- time augmentation. Testing their test-time augmentation scheme on medical image segmentation, they found that it outperformed the single-prediction baseline and drop- out-based multiple predictions. They also found better uncertainty estimation when using test-time augmentation, reducing highly confident but incorrect predictions. Their test-time augmentation method uses a Monte Carlo simulation in order to obtain parameters for different augmentations such as flipping, scaling, rotation, and transla- tions, as well as noise injections.\n",
      "INFO:root:Narrative text extracted: Test-time augmentation can be found in the Alexnet paper [1], which applies CNNs to the ImageNet dataset. In their experiments, they average the predictions on ten ran- domly cropped patches. These patches consist of one extracted from the center, four corner croppings, and the equivalent regions on the horizontally flipped images. These predictions are averaged to form the final output. He et al. [3] use the same 10-crop test- ing procedure to evaluate their ResNet CNN architecture (Fig. 32).\n",
      "INFO:root:Narrative text extracted: Fig. 32 Impact of test-time data augmentation for skin lesion classification [122]\n",
      "INFO:root:Narrative text extracted: Perez et al. [122] present a study on the effectiveness of test-time augmentation with many augmentation techniques. These augmentations tested include color augmen- tation, rotation, shearing, scaling, flipping, random cropping, random erasing, elastic, mixing, and combinations between the techniques. Table 9 shows the higher perfor- mance achieved when augmenting test images as well as training images. Matsunaga et al. [123] also demonstrate the effectiveness of test-time augmentation on skin lesion classification, using geometric transformations such as rotation, translation, scaling, and flipping.\n",
      "INFO:root:Narrative text extracted: The impact of test-time augmentation on classification accuracy is another mechanism for measuring the robustness of a classifier. A robust classifier is thus defined as having a low variance in predictions across augmentations. For example, a prediction of an image should not be much different when that same image is rotated 20°. In their experiments searching for augmentations with Reinforcement Learning, Minh et al. [117] measure robustness by distorting test images with a 50% probability and contrasting the accu- racy on un-augmented data with the augmented data. In this study, the performance of the baseline model decreases from 74.61 to 66.87% when evaluated on augmented test images.\n",
      "INFO:root:Narrative text extracted: Some classification models lie on the fence in terms of their necessity for speed. This suggests promise in developing methods that incrementally upgrade the confidence of prediction. This could be done by first outputting a prediction with little or no test- time augmentation and then incrementally adding test-time augmentations to increase the confidence of the prediction. Different Computer Vision tasks require certain con- straints on the test-time augmentations that can be used. For example, image recogni- tion can easily aggregate predictions across warped images. However, it is difficult to aggregate predictions on geometrically transformed images in object detection and semantic segmentation.\n",
      "INFO:root:Title extracted: Curriculum learning\n",
      "INFO:root:Narrative text extracted: Aside from the study of Data Augmentation, many researchers have been interested in trying to find a strategy for selecting training data that beats random selection. In the context of Data Augmentation, research has been published investigating the rela- tionship between original and augmented data across training epochs. Some research\n",
      "INFO:root:Title extracted: Table 10 Comparison of resolution across three very popular open-source image datasets\n",
      "INFO:root:Title extracted: Dataset\n",
      "INFO:root:Title extracted: Resolution\n",
      "INFO:root:Narrative text extracted: MNIST handwritten digits\n",
      "INFO:root:Title extracted: ImageNet\n",
      "INFO:root:Narrative text extracted: suggests that it is best to initially train with the original data only and then finish training with the original and augmented data, although there is no clear consensus.\n",
      "INFO:root:Narrative text extracted: In the SamplePairing [65] study, one epoch on ImageNet and 100 epochs on other datasets are completed without SamplePairing before mixed image data is added to the training. Once the SamplePairing images are added to the training set, they run in cycles between 8:2 epochs, 8 with SamplePairing images, 2 without. Jaderberg et al. [124] train exclusively with synthetic data for natural scene text recognition. The synthetic data produced the training data by enumerating through different fonts and augmentations. This produced sets of training images for size 50 k and 90 k lexicons. Mikolajczyk and Grochowski [72] draw comparisons from transfer learning. They suggest that training on augmented data to learn the initial weights of a deep convolutional network is similar to transferring weights trained on other datasets such as ImageNet. These weights are then fine-tuned only with the original training data.\n",
      "INFO:root:Narrative text extracted: Curriculum learning decisions are especially important for One-Shot Learning sys- tems such as FaceNet, presented by Schroff et al. [125]. It is important to find faces which are somewhat similar to the new face such that the learned distance function is actually useful. In this sense, the concept of curriculum learning shares many similarities with adversarial search algorithms or learning only on hard examples.\n",
      "INFO:root:Narrative text extracted: Curriculum learning, a term originally coined by Bengio et al. [126], is an applicable concept for all Deep Learning models, not just those constrained with limited data. Plot- ting out training accuracy over time across different initial training subsets could help reveal patterns in the data that dramatically speed up training time. Data Augmentation constructs massively inflated training from combinations such as flipping, translating, and randomly erasing. It is highly likely that a subset exists in this set such that training will be faster and more accurate.\n",
      "INFO:root:Title extracted: Resolution impact\n",
      "INFO:root:Narrative text extracted: Another interesting discussion about Data Augmentation in images is the impact of res- olution. Higher resolution images such as HD (1920 × 1080 × 3) or 4 K (3840 × 2160 × 3) require much more processing and memory to train deep CNNs. However, it seems intuitive that next-generation models would be trained on higher resolution images. Many current models downsample images from their original resolution to make the classification problem computationally more feasible. However, sometimes this down- sampling causes information loss within the image, making image recognition more dif- ficult (Table 10).\n",
      "INFO:root:Narrative text extracted: It is interesting to investigate the nature of this downsampling and resulting perfor- mance comparison. Wu et al. [127] compare the tradeoff between accuracy and speed\n",
      "INFO:root:Narrative text extracted: Fig. 33 Classifications of the Image to the right by different resolution models trained by Wu et al. [127]\n",
      "INFO:root:Narrative text extracted: when downsampling images to different resolutions. The researchers found that com- posing an ensemble of models trained with high and low-resolution images performed better than any one model individually. This ensemble prediction is found by averag- ing the softmax predictions. The models trained on 256 × 256 images and 512 × 512 images achieve 7.96% and 7.42% top-5 error rates, respectively. When aggregated they achieved a lower top-5 error rate of 6.97%. Therefore, different downsampled images can be viewed as another Data Augmentation scheme (Fig. 33).\n",
      "INFO:root:Narrative text extracted: With the advance of Super-Resolution Convolutional Neural Networks presented by Chong et al. [128] or SRGANs, Super-Resolution Generative Adversarial Networks, presented by Ledig et al. [129], it is interesting to consider if upsampling images to an even higher resolution would result in better models. Quality upsampling on CIFAR-10 images from even 32 × 32 × 3 to 64 × 64 × 3 could lead to better and more robust image classifiers.\n",
      "INFO:root:Narrative text extracted: Resolution is also a very important topic with GANs. Producing high resolution out- puts from GANs is very difficult due to issues with training stability and mode collapse. Many of the newer GAN architectures such as StackGAN [130] and Progressively-Grow- ing GANs [34] are designed to produce higher resolution images. In addition to these architectures, the use of super-resolution networks such as SRGAN could be an effective technique for improving the quality of outputs from a DCGAN [91] model. Once it is practical to produce high resolution outputs from GAN samples, these outputs will be very useful for Data Augmentation.\n",
      "INFO:root:Title extracted: Final dataset size\n",
      "INFO:root:Narrative text extracted: A necessary component of Data Augmentation is the determination of the final data- set size. For example, if all images are horizontally flipped and added to the dataset, the resulting dataset size changes from N to 2N. One of the main considerations with respect to final dataset size is the additional memory and compute constraints associ- ated with augmenting data. Practitioners have the choice between using generators which transform data on the fly during training or transforming the data beforehand and\n",
      "INFO:root:Narrative text extracted: storing it in memory. Transforming data on the fly can save memory, but will result in slower training. Storing datasets in memory can be extremely problematic depending on how heavily the dataset size has been inflated. Storing augmented datasets in memory is especially problematic when augmenting big data. This decision is generally categorized as online or offline data augmentation, (with online augmentation referring to on the fly augmentations and offline augmentation referring to editing and storing data on the disk).\n",
      "INFO:root:Narrative text extracted: In the design of a massively distributed training system, Chilimbi et al. [131] augment images before training to speed up image serving. By augmenting images in advance, the distributed system is able to request and pre-cache training batches. Augmentations can also be built into the computational graph used to construct Deep Learning models and facilitate fast differentiation. These augmentations process images immediately after the input image tensor.\n",
      "INFO:root:Narrative text extracted: Additionally, it is also interesting to explore a subset of the inflated data that will result in higher or similar performance to the entire training set. This is a similar concept to curriculum learning, since the central idea is to find an optimal ordering of training data. This idea is also very related to final dataset size and the considerations of transforma- tion compute and available memory for storing augmented images.\n",
      "INFO:root:Narrative text extracted: Alleviating class imbalance with Data Augmentation\n",
      "INFO:root:Narrative text extracted: Class imbalance is a common problem in which a dataset is primarily composed of examples from one class. This could manifest itself in a binary classification problem such that there is a clear majority-minority class distinction, or in multi-class classifi- cation in which there is one or multiple majority classes and one or multiple minority classes. Imbalanced datasets are harmful because they bias models towards majority class predictions. Imbalanced datasets also render accuracy as a deceitful performance metric. Buda et al. [132] provide a systematic study specifically investigating the impact of imbalanced data in CNNs processing image data. Leevy et al. [27] cover many Data- level and Algorithm-level solutions to class imbalance in big data in general. Data Augmentation falls under a Data-level solution to class imbalance and there are many different strategies for implementation.\n",
      "INFO:root:Narrative text extracted: A naive solution to oversampling with Data Augmentation would be a simple random oversampling with small geometric transformations such as a 30° rotation. Other sim- ple image manipulations such as color augmentations, mixing images, kernel filters, and random erasing can also be extended to oversample data in the same manner as geo- metric augmentations. This can be useful for ease of implementation and quick exper- imentation with different class ratios. One problem of oversampling with basic image transformations is that it could cause overfitting on the minority class which is being oversampled. The biases present in the minority class are more prevalent post-sampling with these techniques.\n",
      "INFO:root:Narrative text extracted: Oversampling methods based on Deep Learning such as adversarial training, Neu- ral Style Transfer, GANs, and meta-learning schemes can also be used as a more intel- ligent oversampling strategy. Neural Style Transfer is an interesting way to create new images. These new images can be created either through extrapolating style with a for- eign style or by interpolating styles amongst instances within the dataset. Using GANs\n",
      "INFO:root:Narrative text extracted: to oversample data could be another effective way to increase the minority class size while preserving the extrinsic distribution. Oversampling with GANs can be done using the entire minority class as “real” examples, or by using subsets of the minority class as inputs to GANs. The use of evolutionary sampling [133] to find these subsets to input to GANs for class sampling is a promising area for future work.\n",
      "INFO:root:Title extracted: Discussion\n",
      "INFO:root:Narrative text extracted: The interesting ways to augment image data fall into two general categories: data warp- ing and oversampling. Many of these augmentations elucidate how an image classifier can be improved, while others do not. It is easy to explain the benefit of horizontal flip- ping or random cropping. However, it is not clear why mixing pixels or entire images together such as in PatchShuffle regularization or SamplePairing is so effective. Addi- tionally, it is difficult to interpret the representations learned by neural networks for GAN-based augmentation, variational auto-encoders, and meta-learning. CNN visuali- zation has been led by Yosinski et al. [134] with their deep visualization method. Hav- ing a human-level understanding of convolutional networks features could greatly help guide the augmentation process.\n",
      "INFO:root:Narrative text extracted: Manipulating the representation power of neural networks is being used in many interesting ways to further the advancement of augmentation techniques. Traditional hand-crafted augmentation techniques such as cropping, flipping, and altering the color space are being extended with the use of GANs, Neural Style Transfer, and meta-learn- ing search algorithms.\n",
      "INFO:root:Narrative text extracted: Image-to-image translation has many potential uses in Data Augmentation. Neural Style Transfer uses neural layers to translate images into new styles. This technique not only utilizes neural representations to separate ‘style’ and ‘content’ from images, but also uses neural transformations to transfer the style of one image into another. Neural Style Transfer is a much more powerful augmentation technique than traditional color space augmentations, but even these methods can be combined together.\n",
      "INFO:root:Narrative text extracted: An interesting characteristic of these augmentation methods is their ability to be com- bined together. For example, the random erasing technique can be stacked on top of any of these augmentation methods. The GAN framework possesses an intrinsic property of recursion which is very interesting. Samples taken from GANs can be augmented with traditional augmentations such as lighting filters, or even used in neural network augmentation strategies such as Smart Augmentation or Neural Augmentation to cre- ate even more samples. These samples can be fed into further GANs and dramatically increase the size of the original dataset. The extensibility of the GAN framework is amongst many reasons they are so interesting to Deep Learning researchers.\n",
      "INFO:root:Narrative text extracted: Test-time augmentation is analogous to ensemble learning in the data space. Instead of aggregating the predictions of different learning algorithms, we aggregate predictions across augmented images. We can even extend the solution algorithm to parameterize prediction weights from different augmentations. This seems like a good solution for sys- tems concerned with achieving very high performance scores, more so than prediction\n",
      "INFO:root:Narrative text extracted: speed. Determining the effectiveness of test-time augmentation by primarily exploring test-time geometric transformations and Neural Style Transfer, is an area of future work. An interesting question for practical Data Augmentation is how to determine post- augmented dataset size. There is no consensus as to which ratio of original to final dataset size will result in the best performing model. However, imagine using color aug- mentations exclusively. If the initial training dataset consists of 50 dogs and 50 cats, and each image is augmented with 100 color filters to produce 5000 dogs and 5000 cats, this dataset will be heavily biased towards the spatial characteristics of the original 50 dogs and 50 cats. This over-extensive color-augmented data will cause a deep model to overfit even worse than the original. From this anecdote, we can conceptualize the existence of an optimal size for post-augmented data.\n",
      "INFO:root:Narrative text extracted: Additionally, there is no consensus about the best strategy for combining data warping and oversampling techniques. One important consideration is the intrinsic bias in the initial, limited dataset. There are no existing augmentation techniques that can correct a dataset that has very poor diversity with respect to the testing data. All these augmenta- tion algorithms perform best under the assumption that the training data and testing data are both drawn from the same distribution. If this is not true, it is very unlikely that these methods will be useful.\n",
      "INFO:root:Title extracted: Future work\n",
      "INFO:root:Narrative text extracted: Future work in Data Augmentation will be focused on many different areas such as establishing a taxonomy of augmentation techniques, improving the quality of GAN samples, learning new ways to combine meta-learning and Data Augmentation, discov- ering relationships between Data Augmentation and classifier architecture, and extend- ing these principles to other data types. We are interested in seeing how the time-series component in video data impacts the use of static image augmentation techniques. Data Augmentation is not limited to the image domain and can be useful for text, bioinfor- matics, tabular records, and many more.\n",
      "INFO:root:Narrative text extracted: Our future work intends to explore performance benchmarks across geometric and color space augmentations across several datasets from different image recognition tasks. These datasets will be constrained in size to test the effectiveness with respect to limited data problems. Zhang et al. [135] test their novel GAN augmentation technique on the SVHN dataset across 50, 80, 100, 200, and 500 training instances. Similar to this work, we will look to further establish benchmarks for different levels of limited data.\n",
      "INFO:root:Narrative text extracted: Improving the quality of GAN samples and testing their effectiveness on a wide range of datasets is another very important area for future work. We would like to further explore the combinatorics of GAN samples with other augmentation techniques such as applying a range of style transfers to GAN-generated samples.\n",
      "INFO:root:Narrative text extracted: Super-resolution networks through the use of SRCNNs, Super-Resolution Convolu- tional Neural Networks, and SRGANs are also very interesting areas for future work in Data Augmentation. We want to explore the performance differences across archi- tectures with upsampled images such as expanding CIFAR-10 images from 32 × 32 to 64 × 64 to 128 × 128 and so on. One of the primary difficulties with GAN samples is try- ing to achieve high-resolution outputs. Therefore, it will be interesting to see how we can use super-resolution networks to achieve high-resolution such as DCGAN samples\n",
      "INFO:root:Narrative text extracted: inputted into an SRCNN or SRGAN. The result of this strategy will be compared with the performance of the Progressively Growing GAN architecture.\n",
      "INFO:root:Narrative text extracted: Test-time augmentation has the potential to make a massive difference in Computer Vision performance and has not been heavily explored. We want to establish bench- marks for different ensembles of test-time augmentations and investigate the solution algorithms used. Currently, majority voting seems to be the dominant solution algo- rithm for test-time augmentation. It seems highly likely that test-time augmentation can be further improved if the weight of each augmented images prediction is further parameterized and learned. Additionally, we will explore the effectiveness of test-time augmentation on object detection, comparing color space augmentations and the Neural Style Transfer algorithm.\n",
      "INFO:root:Narrative text extracted: Meta-learning GAN architectures is another exciting area of interest. Using Reinforce- ment Learning algorithms such as NAS on the generator and discriminator architectures seem very promising. Another interesting area of further research is to use an evolu- tionary approach to speed up the training of GANs through parallelization and cluster computing.\n",
      "INFO:root:Narrative text extracted: Another important area of future work for practical integration of Data Augmentation into Deep Learning workflows is the development of software tools. Similar to how the Tensorflow [136] system automates the back-end processes of gradient-descent learn- ing, Data Augmentation libraries will automate preprocessing functions. The Keras [137] library provides an ImageDataGenerator class that greatly facilitates the implementa- tion of geometric augmentations. Buslaev et al. presented another augmentation tool they called Albumentations [138]. The development of Neural Style Transfer, adversar- ial training, GANs, and meta-learning APIs will help engineers utilize the performance power of advanced Data Augmentation techniques much faster and more easily.\n",
      "INFO:root:Title extracted: Conclusion\n",
      "INFO:root:Narrative text extracted: This survey presents a series of Data Augmentation solutions to the problem of overfit- ting in Deep Learning models due to limited data. Deep Learning models rely on big data to avoid overfitting. Artificially inflating datasets using the methods discussed in this survey achieves the benefit of big data in the limited data domain. Data Augmen- tation is a very useful technique for constructing better datasets. Many augmentations have been proposed which can generally be classified as either a data warping or over- sampling technique.\n",
      "INFO:root:Narrative text extracted: The future of Data Augmentation is very bright. The use of search algorithms com- bining data warping and oversampling methods has enormous potential. The layered architecture of deep neural networks presents many opportunities for Data Augmen- tation. Most of the augmentations surveyed operate in the input layer. However, some are derived from hidden layer representations, and one method, DisturbLabel [28], is even manifested in the output layer. The space of intermediate representations and the label space are under-explored areas of Data Augmentation with interesting results. This survey focuses on applications for image data, although many of these techniques and concepts can be expanded to other data domains.\n",
      "INFO:root:Narrative text extracted: Data Augmentation cannot overcome all biases present in a small dataset. For exam- ple, in a dog breed classification task, if there are only bulldogs and no instances of\n",
      "INFO:root:Narrative text extracted: golden retrievers, no augmentation method discussed, from SamplePairing to AutoAug- ment to GANs, will create a golden retriever. However, several forms of biases such as lighting, occlusion, scale, background, and many more are preventable or at least dra- matically lessened with Data Augmentation. Overfitting is generally not as much of an issue with access to big data. Data Augmentation prevents overfitting by modifying lim- ited datasets to possess the characteristics of big data.\n",
      "INFO:root:Narrative text extracted: Abbreviations GAN: generative adversarial network; CNN: convolutional neural network; DCGAN: deep convolutional generative adversarial network; NAS: neural architecture search; SRCNN: super-resolution convolutional neural network; SRGAN: super-resolution generative adversarial network; CT: computerized tomography; MRI: magnetic resonance imaging; PET: positron emission tomography; ROS: random oversampling; SMOTE: synthetic minority oversampling technique; RGB: red-green–blue; PCA: principal components analysis; UCI: University of California Irvine; MNIST: Modified National Institute of Standards and Technology; CIFAR: Canadian Institute for Advanced Research; t-SNE: t-distributed stochastic neighbor embedding.\n",
      "INFO:root:Narrative text extracted: Acknowledgements We would like to thank the reviewers in the Data Mining and Machine Learning Laboratory at Florida Atlantic University. Additionally, we acknowledge partial support by the NSF (CNS-1427536). Opinions, findings, conclusions, or recommen- dations in this paper are solely of the authors’ and do not reflect the views of the NSF.\n",
      "INFO:root:Narrative text extracted: Authors’ contributions CS performed the primary literature review and analysis for this work, and also drafted the manuscript. TMK, JLL, RAB, RZ, KW, NS, and RK worked with CS to develop the article’s framework and focus. TMK introduced this topic to CS, and helped to complete and finalize this work. All authors read and approved the final manuscript.\n",
      "INFO:root:Title extracted: Funding Not applicable.\n",
      "INFO:root:Title extracted: Availability of data and materials Not applicable.\n",
      "INFO:root:Narrative text extracted: Competing interests The authors declare that they have no competing interests.\n",
      "INFO:root:Title extracted: Consent for publication Not applicable.\n",
      "INFO:root:Narrative text extracted: Ethics approval and consent to participate Not applicable.\n",
      "INFO:root:Title extracted: Received: 9 January 2019 Accepted: 22 April 2019\n",
      "INFO:root:Title extracted: References\n",
      "INFO:root:Title extracted: deep learning-based medical image segmentation. OpenReview.net. 2018.\n",
      "INFO:root:Narrative text extracted: a system for large-scale machine learning. In: Proceedings of the 12th USENIX symposium on operating system design and implementation (OSDI ‘16), 2016.\n",
      "INFO:root:Narrative text extracted: Publisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"error\": [\n",
      "    {\n",
      "      \"message\": \"invalid object: invalid UUID length: 0\"\n",
      "    }\n",
      "  ],\n",
      "  \"valid\": false\n",
      "}\n",
      "{'error': [{'message': \"invalid text property 'title' on class 'Test': not a string, but []interface {}\"}]}\n",
      "{'error': [{'message': \"invalid text property 'title' on class 'Test': not a string, but []interface {}\"}]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from weaviate.util import generate_uuid5\n",
    "\n",
    "# Define the class for storing PDFs\n",
    "class PDFIngestor:\n",
    "    def __init__(self, directory, client):\n",
    "        self.directory = Path(directory)\n",
    "        self.client = client\n",
    "        self.extractor = TitleNarrativeExtractor()\n",
    "        self.data_objects = []\n",
    "        self.result=None\n",
    "        self.data_object=None   \n",
    "\n",
    "    def ingest_pdfs(self):\n",
    " \n",
    "    \n",
    "        # Iterate over all PDF files in the directory\n",
    "        for path in self.directory.glob('*.pdf'):\n",
    "            print(f\"Processing {path.name}...\")\n",
    "\n",
    "            # Convert the PDF to text (this is a list of elements with text that will be vectorized)\n",
    "            elements = partition_pdf(filename=path)\n",
    "\n",
    "            # Extract data using TitleNarrativeExtractor\n",
    "            self.extractor.extract_elements(elements)\n",
    "\n",
    "\n",
    "            self.data_object = {\n",
    "                                    \"source\": path.name,\n",
    "                                    \"Title\": [' '.join(str(title) for title in self.extractor.get_titles())],  # Concatenate titles into a single string  # Ensure titles are strings\n",
    "                                    \"content\": [' '.join(str(text) for text in self.extractor.get_narrative_texts())],  # Ensure narrative texts are strings\n",
    "                                    \"path\": str(path)\n",
    "\n",
    "                                    \n",
    "                    }\n",
    "            \n",
    "            self.result = self.client.data_object.validate(\n",
    "                class_name=\"Test\",\n",
    "                    data_object = {\n",
    "                                    \"source\": path.name,\n",
    "                                    \"Title\": ' '.join(str(title) for title in self.extractor.get_titles()),  # Concatenate titles into a single string  # Ensure titles are strings\n",
    "                                    \"content\": ' '.join(str(text) for text in self.extractor.get_narrative_texts()),  # Ensure narrative texts are strings\n",
    "                                    \"path\": str(path)\n",
    "\n",
    "                                    \n",
    "                    },\n",
    "                )\n",
    "            print(json.dumps(self.result, indent=2))\n",
    "\n",
    "            # Add the data object to the batch request\n",
    "            self.data_objects.append(self.data_object)\n",
    "\n",
    "            with client.batch as batch:\n",
    "                for self.data_object in self.data_objects:\n",
    "                    batch.add_data_object(self.data_object, \"Test\", uuid=generate_uuid5(self.data_object))\n",
    "\n",
    "                \n",
    "# Use the class\n",
    "ingestor = PDFIngestor('/Users/ceciliaacosta/ift6759/how-to-ingest-pdfs-with-unstructured/data', client)\n",
    "ingestor.ingest_pdfs()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ift-6758",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
